
Applying citizen science to gene, drug and disease relationship extraction from biomedical abstracts
====================================================================================================
  
  [@wikidata:Q90025338]  
  
Publication date : 01 of February, 2020  

# Highlights

In this article, we introduce the Relationship Extraction Module of the web-based application Mark2Cure (M2C) and demonstrate that citizen scientists can perform RE

--> Mark2Cure - 3.2.2. Community curation and gamified science

We also discuss opportunities for future improvement of this system, as well as the potential synergies between citizen science, manual biocuration and natural language processing.

Information extraction as a process can be divided into a few sub tasks: (i) Named Entity Recognition (NER), (ii) Entity Linking (EL) and (iii) Relationship Extraction (RE).

NER entails identifying specific types of entities within biomedical text [e.g. NGLY1(entity) is a gene(entity type)].

Crowdsourcing through paid microtask platforms to expand the training datasets has proven to have great potential, but questions regarding scalability prompted us to investigate citizen science as a potential avenue for crowdsourcing RE.

In aggregate, nonexperts recruited via a microtask platform could perform relationship annotation on par with expert curation (Dumitrache et al., 2015) provided that the task is appropriately designed (Khare et al., 2015).

Concept pairs within the same concept type are not included because the relationship between concepts of the same concept type tend to be hypernymic relationships (e.g. ‘is a’) which algorithms are good at identifying (Rindflesch and Fiszman, 2003).

Because there are no expert-curated gold standard relationships extraction data available for the PMIDs covered in this experiment, a subset of the computationally derived open dataset, SemMedDB was used for comparative purposes. The PMIDs for the completed concept pairs were used to pull SemMedDB annotations specific to those PMIDs. 

SemMedDB concept annotations are linked to concept-unique identifiers (CUIs) from the Unified Medical Language System (UMLS). The UMLS integrates many biomedical vocabularies and standards (Bodenreider, 2004)

 Beyond six users, the median accuracy does not further increase, suggesting that each relationship task should be annotated by a maximum of six users to maximize accuracy while minimizing redundancy of work done by the community (Fig. 1C).

 For example, the ‘WD’ from an abstract discussing tryptophan-Aspartic Acid repeats (PMID 16609705) was incorrectly annotated by the NER algorithm as a disease (D006527/Wilson’s Disease). The majority response in this case would be to mark the annotation as ‘broken’ or incorrect. However, inquiries with our users have indicated that our guidance here was lacking as a small subset of our users would reason that ‘WD’ is indeed a disease, even if it is not a disease in this particular abstract

 Citizen science is a potential avenue for generating new training datasets for improving automated RE tools, but should not be considered a cheaper version of crowdsourcing.
# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q90025338)  
 * [Wikidata](https://www.wikidata.org/wiki/Q90025338)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q90025338/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q90025338&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1093/BIOINFORMATICS/BTZ678)  
