
Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup.
=================================================================================================
  
  [@wikidata:Q30811199]  

# Highlights

Many biological databases draw much of
their content from a careful curation of this literature.
However, as the volume of literature increases, the burden
of curation increases. Text mining may provide useful tools
to assist in the curation process. To date, the lack of
standards has made it impossible to determine whether
text mining techniques are sufficiently mature to be useful.
Results: We report on a Challenge Evaluation task that
we created for the Knowledge Discovery and Data Mining
(KDD) Challenge Cup. We provided a training corpus
of 862 articles consisting of journal articles curated in
FlyBase, along with the associated lists of genes and gene
products, as well as the relevant data fields from FlyBase.
For the test, we provided a corpus of 213 new (‘blind’)
articles; the 18 participating groups provided systems that
flagged articles for curation, based on whether the article
contained experimental evidence for gene expression
products

There has been a growing volume of work in text
mining for biological literature, but until now, there has
been no way to compare results of the different systems
(Hirschman et al., 2002).

The KDD Challenge Cup schedule included a 6 week
period when the training data was made available for
the contestants to build and train a system, followed by
a two week period to complete the running of the test
material. The results were then submitted to MITRE for
final scoring

Another example is that for the paper Tingvall et al.
(2001), FlyBase records that mRNA transcripts of certain
reporter constructs (a construct is a combination of a
reporter gene and a gene of interest) appear in certain parts
of the body. The paper itself never explicitly mentions
any transcript. Instead, the supporting text mentions where
the associated reporter protein is detected. The FlyBase
curators infer the transcripts’ locations from the places
where the protein is detected. Manually finding such
‘evidence’ passages for use in training a system would
have been both time consuming and difficult†
. So wedropped the passage finding requirement.

† Especially since it can require a lot of biology knowledge and our contestants had more of a data-mining background than a biology background.

Overall, 18 teams returned 32 separate submissions for
evaluation (up to 3 per team). There were eight countries
represented, including Japan, Taiwan, Singapore, India,
Israel, UK, Portugal and USA.

We declared a winning team and three honorable mention
teams. The teams used a variety of approaches. The
winning team (Regev et al., 2003) used an information
extraction approach with manually constructed rules that
matched against patterns deemed of interest.

One lesson we learned from running this contest is that
access to the literature itself is a complex matter. Abstracts
of papers are fairly easy to obtain via PubMed/Medline
(http://www.ncbi.nlm.nih.gov/entrez/query.fcgi). However, many of the results of interest to the FlyBase curators
are only described in the full paper, and not in the abstract.

One of our goals in running this evaluation was to
evaluate the evaluation. For this, we defined three criteria:

• The evaluation should be repeatable and affordable.
This should include a reusable training data set, costeffective preparation of ‘gold standard’ data for test
and repeatable scoring procedures that are easy to run
and easy to understand.

• The evaluation must attract participants. This means
that it needs to be a problem of importance to
biologists, but also accessible to researchers in text
mining teamed with biologists.

• The task must be tractable, but should also push the
state of the art. If the task is well chosen, groups
will demonstrate that they are on the path to the
development of a useful capability.
Our assessment of the KDD evaluation was that it was
successful along all of these dimensions

However, because of the venue (KDD), we attracted mostly researchers in text mining, rather than biologists
# Comments

## Tags
1.1.1
3.2.2

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q30811199)  
 * [Wikidata](https://www.wikidata.org/wiki/Q30811199)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q30811199/P921%3BP4510)  
