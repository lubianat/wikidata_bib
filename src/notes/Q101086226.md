
KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis
===================================================================
  
  [@wikidata:Q101086226]  
  
Publication date : 01 of January, 2020  

# Highlights

While KGs have become a mainstream technology, the
RDF/SPARQL-centric toolset for operating with them at scale is heterogeneous, difficult to integrate and only covers a subset of the operations
that are commonly needed in data science applications. In this paper we
present KGTK, a data science-centric toolkit designed to represent, create, transform, enhance and analyze KGs.

In this paper, we introduce the Knowledge Graph Toolkit (KGTK), a framework for manipulating, validating, and analyzing large-scale KGs.

– The KGTK file format, which allows representing KGs as hypergraphs.
This format unifies the Wikidata data model [24] based on items, claims,
qualifiers, and references; property graphs that support arbitrary attributes
on nodes and edges; RDF-Schema-based graphs such as DBpedia [1]; and
general purpose RDF graphs with various forms of reification. The KGTK
format uses tab-separated values (TSV) to represent edge lists, making it
easy to process with many off-the-shelf tools.

A comprehensive validator and data cleaning module to verify compliance with the KGTK format

Import modules to transform different formats into KGTK, including NTriples [21], Wikidata qualified terms, and ConceptNet [22].

Graph querying and analytics modules to compute centrality measures,
connected components, and text-based graph embeddings using state-ofthe-art language models: RoBERTa [15], BERT [5], and DistilBERT [19].
Common queries, such as computing the set of nodes reachable from other
nodes, are also supported.

We extracted
all the items and statements for the 30,000 articles in the CORD-19 corpus [25]
that were present in Wikidata at the time of extraction, added all Wikidata
articles, authors, and entities mentioned in the BLENDER corpus, homogenized
the data to fix inconsistencies (e.g., empty values), created nodes and statements
for entities that were absent in Wikidata, incorporated metrics such as PageRank
for each KG node, and exported the output in both RDF and Neo4J.

 Graph analytics operations followed, such as
computing centrality measures in order to support identification of key articles,
people or substances,15 or generation of various embeddings to recommend relevant literature associated with an entity.16 The resulting graphs were deployed
as SPARQL endpoints, or exported as RDF dumps, CSV, or JSON files.


node1 label node2 creator source id
"Moe" rdf:type Person "Hans" Wikipedia E1


The first line of a KGTK file declares the headers for the document. The reserved words node1, label and node2 are used to describe the subject, property
and object being described, while creator and source are optional qualifiers for
each statement that provide additional provenance information about the creator of a statement and the original source. Note that the example is not using
namespace URIs for any nodes and properties, as they are not needed for local
KG manipulation.

Bob wants to extract a subset of Wikidata that contains only
edges of the ‘member of’ (P463) property, and strip a set of columns that are
not relevant for his use case ($ignore col), such as id and rank. While doing
so, Bob would also like to clean any erroneous edges. On the clean subset, he
would compute graph statistics, including PageRank values and node degrees.
Here is how to perform this functionality in KGTK (after Wikidata is already
converted to a KGTK file called wikidata.tsv by import-wikidata):
kgtk filter -p ’ ; P463 ; ’ / clean_data / remove-columns -c "$ignore_cols" wikidata.tsv > graph.tsv kgtk graph-statistics --directed --degrees --pagerank graph.tsv


Fig. 2. SPARQL query and visualization of the CORD-19 use case, illustrating the
use of the Wikidata infrastructure using our KG that includes a subset of Wikidata
augmented with new properties such as “mentions gene” and “pagerank”

We believe KGTK will have a significant impact within and beyond the Semantic Web community by helping users to easily perform usual data science
operations on large KGs. 

The primary limitation of KGTK lies in its functionality coverage. The main
focus so far has been on supporting basic operations for manipulating KGs, and
therefore KGTK does not yet incorporate powerful browsing and visualization
tools, or advanced tools for KG identification tasks such as link prediction, entity
resolution, and ontology mapping
Additionally, it is unclear how to extend SPARQL to support functionalities such
as computing embeddings or node centrality. A scalable alternative to SPARQL
is Linked Data Fragments (LDF) [23

The recent developments towards supporting triple annotations with
RDF* [9] provide support for qualifiers; yet, this format is still in its infancy
and we expect it to inherit the challenges of RDF, as described before.

# Comments



## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q101086226)  
 * [Wikidata](https://www.wikidata.org/wiki/Q101086226)  
 * [Author Disambiguator](https://author-
disambiguator.toolforge.org/work_item_oauth.php?id=Q101086226&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1007/978-3-030-62466-8_18)  
