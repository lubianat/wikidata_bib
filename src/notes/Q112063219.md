

To generate better representations of biological knowledge, we propose STonKGs, a Sophisticated Transformer trained on biomedical text and Knowledge Graphs (KGs). This multimodal Transformer uses combined input sequences of structured information from KGs and unstructured text data from biomedical literature to learn joint representations in a shared embedding space. 

. For instance, certain proteins or chemicals may exclusively interact with others in a specific tissue or cell type (Stacey et al., 2018), or specific biochemical reactions may only take place under certain conditions. Consequently, to exploit the biomedical knowledge stored in both structured and unstructured formats, it is crucial to study each relation in the relevant context it was observed in. While networks often lack this contextual information due to their inherent degree of abstraction (Saqi et al., 2019), unstructured text contains context at the expense of explicit logical structure. Thus, the complementary strengths from both sources could be leveraged to enable a more complete, context-specific and actionable representation of biological knowledge.

Using this dataset, we benchmark STonKGs against two baseline models [i.e. BioBERT (Lee et al., 2019) and node2vec (Grover and Leskovec, 2016)] in a transfer learning setting on eight different fine-tuning tasks corresponding to distinct biological applications.

These weights are learnable parameters used to calculate weighted average representation of a given entity (i.e. text token or node of the input sequence) based on the embedding vectors of its surrounding entities from both modalities. As a result, the calculated representation of each entity contains contextual information from the KG and text input.

An overview on the tasks as well as their respective summary statistics can be found in Supplementary Table S2. The distribution of classes of the fine-tuning tasks can be found in Supplementary Figure S2.

 Finally, the source code and the pre-trained model are available at https://github.com/stonkgs, enabling to leverage both the pre-trained STonKGs model as well as the overall model architecture for a variety of additional ML-based tasks that use text and KG data.

  In addition, further tasks with closer resemblance to the expected real-world use-cases (i.e. tasks with a larger number of classes, or a dedicated ‘unknown’ class) can be added to the benchmark to assess the potential of STonKGs for future application scenarios. Finally, an in-depth analysis of the attention weights between the text tokens and KG nodes used in STonKGs could reveal valuable insights about the interdependencies between the two modalities.