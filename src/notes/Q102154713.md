
Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases
=======================================================================================
  
  [@wikidata:Q102154713]  
  
Publication date : 18 of November, 2020  

# Highlights

Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution.

We construct and release a new large-scale, high-quality dataset with 64,331 questions, GRAILQA, and provide evaluation settings for all three levels of generalization.

Therefore, we argue that practical KBQA models should be built with strong generalizability to out-of-distribution questions at test time.

For example, if a model has been trained with questions about relations like "producer", "staged_here" and "capacity", classes like "Theater", and functions like "GE", it should be able to answer complex questions involving all these schema items even though this specific composition is not covered in training. 

Furthermore, a KBQA model may also encounter questions about schema items or even entire domains that are not covered in training at all (e.g., TV_Program and program_created) and needs to generalize in a zero-shot fashion [32].

The dataset covers all the 86 domains in FREEBASE COMMONS, the part of FREEBASE considered to be high-quality and ready for
public use, and most of the classes and relations in those domains.

Finally, we develop multiple quality control mechanisms for crowdsourcing to ensure dataset quality, and the final dataset is contributed by 6,685 crowd workers with highly diverse demographics.

In addition to the dataset, we also study important factors towards stronger generalization and propose a novel KBQA model based on pre-trained language models like BERT [10].

We present the first systematic study on three levels of generalization, i.e., i.i.d., compositional, and zero-shot, for KBQA and discuss their importance for practical KBQA systems.

Denote Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› as the set of schema items in any of the training examples and Sğ‘ that of a question ğ‘. A qualifying test set Q for each level of generalization is defined as:


I.I.D. generalization: âˆ€ğ‘ âˆˆ Q, Sğ‘ âŠ‚ Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. 
In addition, the test questions follow the training distribution, e.g., randomly sampled from the training data. 
â€¢ Compositional generalization: âˆ€ğ‘ âˆˆ Q, Sğ‘ âŠ‚ Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, however, the specific logical form of ğ‘ is not covered in training.
â€¢ Zero-shot generalization: âˆ€ğ‘ âˆˆ Q, âˆƒğ‘  âˆˆ Sğ‘, ğ‘  âˆˆ S \ Sğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›.


It first traverses the KB ontology to generate graph-shaped templates that only consist of classes,
relations, and functions, and then ground certain nodes to compatible entities to generate logical forms in their meaning representation called graph query

The system keeps running until 5 valid paraphrases (Task 2) are obtained for each canonical question.
We pay $0.10 per paraphrase.

 <!-- I'd work for that rate, by the way. But not for long.  -->

GRAILQA 64,331 4,969 86 3,720 1,534 32,585 3,239 i.i.d. + comp.+ zero-shot

In total we have collected 4,969 canonical logical forms and 29,457 paraphrases (including the canonical questions).

 The dataset is contributed by 11 graduate students and 6,685 crowd workers with diverse demographics in terms of age group, education background, and gender (Appendix A)

Common types of surface forms include acronym (â€œFDAâ€ for Food_and_Drug_Administration), last/first name (â€œObamaâ€ for Barack_Obama), commonsense (â€œHer Majesty the Queenâ€ for Elizabeth_II), and colloquial vs. formal (â€œObama vs. Romneyâ€ for
United_States_Presidential_Election_2012)

In addition to graph query, we provide an alternative linearized version of it in S-expressions, which is needed to apply the mainstream sequence-to-sequence (Seq2Seq) neural models [12, 20, 47]

<!-- That is the core challenge -->

Models that build their vocabulary solely from training data (e.g., [47]) would almost certainly fail at zero-shot generalization. 
Question-specific search space pruning is thus more important than it is in the i.i.d. setting.

Before getting to experiments on our GRAILQA dataset, we conduct an experiment on the existing dataset GRAPHQ to show the competitive performance of our model.
Our RANKING model achieves an F1 of 25.0%, which significantly outperforms the prior art SPARQA [37] by 3.5 percent. With this
superior performance, we believe our new model is reasonable to serve as a strong baseline on GRAILQA and support the subsequent investigations on three levels of generalization.

A good KBQA model should be robust to different paraphrases and entity groundings (e.g., should not succeed at â€œWhere is the Trump tower?â€ but fail at â€œWhere is the Tune Hotels?â€)

We also show that GRAILQA could serve as a valuable pre-training corpus for KBQA in general by pre-training RANKING on GRAILQA and testing its transferability to WEBQ, which contains naturallyoccurring questions from Google search log. To adapt RANKING
to WEBQ, we first convert SPARQL queries in WEBQ to our Sexpression logical forms.

There have been an array of datasets for KBQA in recent years. 
  - WEBQ [4] is collected from Google search logs with the answers annotated via crowdsoursing. 
  - SIMPLEQ [6] is another popular KBQA dataset created by sampling individual facts from FREEBASE and annotating them as natural language questions. 
  - ALD [31] and LC-QUAD [40] are popular datasets on DBPEDIA. 
  QALD is manually created by expert annotators, while LC-QUAD first generates SPARQL queries and unnatural canonical questions with templates and have expert annotators paraphrase the canonical questions.

This work is just a starting point towards building more practical KBQA models with stronger generalization.

We are  also interested in experimenting with other pre-trained contextual embeddings such as RoBERTa [27] and BART [25].


emrKBQA contains natural language questions posed by physicians at the Veteranâ€™s Administration (VA), Mayo Clinic and
Cleveland Clinic on patient records (Raghavan et al., 2018).
Figure 3: Mapping between emrKBQA schema entities, attributes and tables (yellow boxes) and columns in
MIMIC (shown in blue). See MIMIC schema for a description of MIMIC table and column names(Johnson et al.,
2016)

Answers in emrKBQA are cell values from a table(s) in the KB. Broadly the answer categories in emrKBQA are Test, Medication, Allergy, Therapeutic Procedures, Conditions and Smoking. (4) Logical forms generated from the same schema as emrQA, allowing the schema to be a unifying factor across structured and unstructured QA. This allows for future updates in a uniform manner.

The benefit of a structured dataset such as MIMIC is that explicit values are captured well in tables. Unstructured data may have the answer implicitly stated and may have to be inferred.

# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q102154713)  
 * [Wikidata](https://www.wikidata.org/wiki/Q102154713)  
 * [Author Disambiguator](https://author-
disambiguator.toolforge.org/work_item_oauth.php?id=Q102154713&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [Full text URL](https://arxiv.org/pdf/2011.07743.pdf)  
 * [arXiv ID](https://arxiv.org/pdf/2011.07743.pdf)  
