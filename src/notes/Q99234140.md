
How we learnt to stop worrying and love web scraping
====================================================
  
  [@wikidata:Q99234140]  

# Highlights

With a few days of work, we were able to write a computer program that could quickly, efficiently and reproducibly collect all the PDFs and create a spreadsheet that documented each case.

Such a tool is called a ‘web scraper.

A common scraping task involves iterating over every possible URL from www.example.com/data/1 to www.example.com/data/100 (sometimes called ‘crawling’) and storing what you need from each page without the risk of human error during extraction.


off-the-shelf browser extensions such as webscraper.io let you click on the elements of the page that contain the data that you’re interested in

Requests and BeautifulSoup work well together in Python; for R, try rvest.

Can you get the data an easier way? 
Luckily, ClinicalTrials.gov makes their full dataset available for download

If there’s no bulk download available, check to see whether the website has an application programming interface (API).

A few websites simply don’t want to be scraped and are built to discourage it.

You might be able to use what you scrape, but it’s worth checking that you can also legally share it. 


# Comments
