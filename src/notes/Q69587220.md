
Construction of the Literature Graph in Semantic Scholar
========================================================
  
  [@wikidata:Q69587220]  

# Highlights

We describe a deployed scalable system for organizing published scientific litera- ture into a heterogeneous graph to facili- tate algorithmic manipulation and discov- ery. The resulting literature graph consists of more than 280M nodes, representing pa- pers, authors, entities and various interac- tions between them (e.g., authorships, cita- tions, entity mentions).

--> man, that is HUGE

The methods described in this pa- per are used to enable semantic features in www.semanticscholar.org 

researchers remain unable to answer simple ques- tions such as:
? What is the percentage of female subjects in depression clinical trials?
? Which of my co-authors published one or more papers on coreference resolution?
? Which papers discuss the effects of Ranibizumab on the Retina?

In this paper, we focus on the problem of ex-tracting structured data from scientific documents, which can later be used in natural language inter- faces (e.g., Iyer et al., 2017) or to improve ranking of results in academic search

We reduce literature graph construction into familiar NLP tasks such as sequence labeling, entity linking and relation extraction, and address some of the impractical assumptions commonly made in the standard formulations of these tasks.

The literature graph is a property graph with di- rected edges. Unlike Resource Description Frame- work (RDF) graphs, nodes and edges in property graphs have an internal structure which is more suitable for representing complex data types such as papers and entities.

Entity mentions. Each node of this type rep-
resents a textual reference of an entity in one of the papers, with attributes such as ‘mention text’, ‘context’, and ‘confidence’. We describe how we populate the 237M mentions in the literature graph in §4.1

Although some publishers provide sufficient metadata about their papers, many papers are pro- vided with incomplete metadata. Also, papers ob- tained via web-crawling are not associated with any metadata. To fill in this gap, we built the Sci- enceParse system to predict structured data from the raw PDFs using recurrent neural networks (RNNs).2 For each paper, the system extracts the paper title, list of authors, and list of references; each reference consists of a title, a list of authors, a venue, and a year.

--> Nice.

4 Entity Extraction and Linking
In the previous section, we described how we popu- late the backbone of the literature graph, i.e., paper nodes, author nodes and citation edges. Next, we discuss how we populate mentions and entities in the literature graph using entity extraction and link- ing on the paper text.

III. Off-the-shelf: uses existing libraries, namely (Ferragina and Scaiella, 2010, TagMe)5 and (Demner-Fushman et al., 2017, MetaMap Lite)6, with minimal post-processing to extract and link entities to the KB

Unlike most formula- tions of named entity recognition problems (NER), we do not identify the entity type (e.g., protein, drug, chemical, disease) for each mention since the output mentions are further grounded in a KB with further information about the entity (including its type), using an entity linking module.

Grounding the entity mentions in a manually-curated KB also increases user confi- dence in automated predictions. We use two KBs: UMLS: The UMLS metathesaurus integrates in- formation about concepts in specialized ontologies in several biomedical domains, and is funded by the U.S. National Library of Medicine. 
DBpedia: DBpedia provides access to structured information in Wikipedia. Rather than including all Wikipedia pages, we used a short list ofWikipedia categories about CS and included all pages up to depth four in their trees in order to exclude irrele- vant entities, e.g., “Lord of the Rings” in DBpedia

--> You could have used Wikidata :( 

  ented as entity ID D016571 in the MESH ontology, and represented as page ID ‘21523’ in DBpedia. Ontology matching is the problem of identifying semantically-equivalent entities across KBs or ontologies.

The convenience of grounding entities in a hand-curated KB comes at the cost of limited coverage. Introduction of new concepts and relations in the scientific literature occurs at a faster pace than KB curation, resulting in a large gap in KB coverage of scientific concepts. In order to close this gap, we need to develop mod- els which can predict textual relations as well as detailed concept descriptions in scientific papers. For the same reasons, we also need to augment the relations imported from the KB with relations extracted from text. Our approach to address both entity and relation coverage is based on distant su- pervision (Mintz et al., 2009). In short, we train two models for identifying entity definitions and relations expressed in natural language in scientific documents, and automatically generate labeled data for training these models using known definitions and relations in the KB

In order to help future research efforts, we make
the following resources publicly available: meta- data for over 20 million papers,10 meaningful cita- tions dataset,11 models for figure and table extrac- tion,12 models for predicting citations in a paper draft 13 and models for extracting paper metadata,14 among other resources.1


# Comments

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q69587220)  
 * [Wikidata](https://www.wikidata.org/wiki/Q69587220)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q69587220/P921%3BP4510)  
