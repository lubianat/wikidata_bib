
Web scraping technologies in an API world
=========================================
  
  [@wikidata:Q86694255]  

# Highlights

Web data scraping, one of the oldest techniques for extracting Web contents, is still in position to offer a valid and valuable service to a wide range of bioinformatics applications, ranging from simple extraction robots to online meta-servers.

The main focus is set on showing how straightforward it is today to set up a data scraping pipeline, with minimal programming effort, and answer a number of practical needs.

- Site access: The Web data scraper establishes com- munication with the target Web site through the HTTP protocol.
- HTML parsing and contents extraction: Once the HTML document is retrieved, the Web data scra- per may extract the contents of interest

Scraping is more robust when the site implements semantic markup, such as Microformats (http://microformats.org) or Microdata (http://www.whatwg.org)

--> Semantic all the way.

Output building:
(typically XML or CSV files)

Similarly, the BeautifulSoup (http://www.crummy. com/software/BeautifulSoup/) is a Python HTML parsing library, which can be combined with language native support for HTTP connections

--> Mentions Java, Perl and PHP which are a bit old fashioned for todays bioinformaticians.

Scraping frameworks present a more integrative solution. For example, Scrapy (http://scrapy.org) is a powerful Web scraping framework for Python, where robots are defined as classes inheriting from BaseSpider class, which defines a set of ‘starting urls’ and a ‘parse’ function called at each Web iteration. Web pages are automatically parsed and Web con- tents are extracted using XPath expressions

--> Scrapy is cool

Manual screening is time- consuming, demands expertise in the field and, still, it is prone to miss valuable details. Information is typically scattered across institutional and laboratory Web sites, and may be highlighted only in journal articles or conference proceedings.


# Comments
