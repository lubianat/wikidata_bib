
Deep neural semantic parsing: translating from natural language into SPARQL
===========================================================================
  
  [@wikidata:Q107297542]  
  

# Highlights

While natural language has well defined vector representation
methods that use a very large volume of texts, formal languages, like SPARQL queries, suffer
from lack of suitable methods for vector representation. In the first contribution we improve the
representation of SPARQL vectors. We start by obtaining an alignment matrix between the two
vocabularies, natural language and SPARQL terms, which allows us to refine a vectorial represen-
tation of SPARQL items. 

Semantic parsing can be defined as the process of mapping natural language sentences into
a machine-interpreted, formal representation of its meaning. The idea of constructing a formal
representation to a sentence in natural language is old, Montague grammar (Bach, 1979, Rodman,
1972) was considered a landmark of its time (Bach, 1979). Richard Montague has developed a whole
theory where it makes a relation between the semantics of natural language and its grammar, that
is is based on formal logic, especially higher-order predicate logic and lambda calculus. One of
the most important features of the model developed by Montague is the principle of semantic
compositionality, that is, the meaning of the whole is a function of the meanings of its parts and
their mode of syntactic combination. Richard Montague’s work inspired a number of other successful
approaches.


The objective of this project is to contribute to the evolution of models of semantic parsing that
do not rely on handcrafted rules, high-quality lexicons, manually-built templates or other handmade
complex structures.

The work (Krishnamurthy et al., 2017) presents a semantic parsing model to answer questions
composed in semi-structured tables of Wikipedia. In addition to ensuring well-structured prediction,
the authors also have a solution to the problem of entity identification. The authors depart from the
standard LSTM encoder decoder architecture. The input of the network are questions in natural
language and a context to which they must be answered. The encoder is a bidirectional LSTM
network, where natural language vectors are concatenated with vectors that represent entity link.
The authors create a special grammar called “Type-Constrained Grammar”, with this grammar it is
possible to maintain a state at each step of the decoder. Briefly, decoder is an LSTM that outputs a
distribution over grammar actions using an attention mechanism over the encoded question tokens.
# Comments

More focused on neural networks and machine learning than the linguistic-to-SPARQL path

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q107297542)  
 * [Wikidata](https://www.wikidata.org/wiki/Q107297542)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q107297542/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q107297542&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.11606/T.45.2019.TDE-01042019-101602)  
