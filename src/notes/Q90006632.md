
BioBERT: a pre-trained biomedical language representation model for biomedical text mining
==========================================================================================
  
  [@wikidata:Q90006632]  
  
Publication date : 01 of February, 2020  

# Highlights

We  introduce  BioBERT  (Bidirectional  Encoder  Representations  from  Transformers  for  Biomedical  TextMining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora.With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-artmodels in a variety of biomedical text mining tasks when pre-trained on biomedical corpor

Our analysis results  show that pre-training BERT on biomedical  corpora helps  it tounderstand complex biomedical texts.

n this study, we hypothesize that current state-of-the-art wordrepresentation models such as BERT need to be trained on biomed-ical  corpora  to  be  effective  in   biomedical  text  mining  tasks.

BioBERT  is  the  first  domain-specific  BERT  based  model  pre-trained  on  biomedical  corpora  for  23  days  on  eight  NVIDIAV100 GPUs.


BERT (Devlinet al., 2019) is a contextualized word representa-tion  model  that  is  based  on  a  masked  language  model  and  pre-trained using bidirectional transformers (Vaswaniet al., 2017). Dueto the nature of language modeling where future words cannot beseen,  previous  language  models  were  limited  to  a  combination  oftwo unidirectional language models (i.e. left-to-right and right-to-left). BERT uses a masked language model that predicts randomlymasked words in a sequence, and hence can be used for learning bi-directional representations

For  tokenization,  BioBERT  uses  WordPiece  tokenization  (Wuet al.,  2016),  which  mitigates  the  out-of-vocabulary  issue.  WithWordPiece tokenization, any new words can be represented by fre-quent subwords (e.g.ImmunoglobulinÂ¼>I ##mm ##uno ##g ##lo##bul ##in).

The results  of NER  are  shown  inTable  6.  First,  we  observe thatBERT, which was pre-trained on only the general domain corpus isquite effective, but the micro averaged F1 score of BERT was lower(2.01 lower) than that of the state-of-the-art models. On the otherhand, BioBERT achieves higher scores than BERT on all the data-sets



# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q90006632)  
 * [Wikidata](https://www.wikidata.org/wiki/Q90006632)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q90006632/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q90006632&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
