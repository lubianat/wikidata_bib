
The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge
=========================================================================
  
  [@wikidata:Q118188004]  
  
Publication date : 04 of May, 2023  

# Highlights

Question answering (QA) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as DBpedia and Wikidata. 

<!-- Nice description of Wikidata as an encyclopedic knowledge graph. -->

We present SciQA a scientific QA benchmark for scholarly knowledge. 

The benchmark leverages the Open Research Knowledge Graph (ORKG) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. 

Based on two preliminary evaluations, we show that the resulting SciQA benchmark represents a challenging task for next-generation QA systems. 
This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data (QALD) Challenge.

Currently, a new type of knowledge graph, called research knowledge graphs, is emerging whose contents are bibliographic metadata and scientific elements, such as ideas, theories, approaches, and claims as they are conveyed in scholarly contributions4,5 or OMICS data structures for personalized medicine6


The benchmark leverages the Open Research Knowledge Graph (ORKG)4,7 (https://orkg.org) currently comprising almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields.


In the best-performing configuration (ùêΩùëéùëüùë£ùëñùë†ùëãùêøùëÜ2 the proof-of-concept implementation of JarvisQA was able to answer 52 questions with 12 correct answers. ChatGPT provided answers for 63 questions, of which only 14 answers were correct. These low numbers substantiate that answering questions about scholarly knowledge is a challenge for current QA systems and LLMs12.

<!-- Training does not cover it well, but hey, maybe it is not 'knowledge', but just "reports", "information" at best -->

The dataset SIMPLEQUESTIONS16 also targets Freebase. It was created manually by English-speaking annotators and consists only of factoid questions. It is much larger than WEBQUESTIONS, containing 108,442 simple questions paired with their corresponding answers and explanations. Diefenbach et al.17 created the benchmark SIMPLEQUESTIONSWIKIDATA by converting SIMPLEQUESTIONS to target Wikidata.

The QALD- 10 benchmark, generated for testing in the latest challenge (NLIWoD, ESWC2022), contains 394 manually created Wikidata-based questions of varied complexity and each is annotated with a manually specified SPARQL query and its output

The ORKG4 is a research knowledge graph that includes semantic descriptions of research articles and accompanying services (https://orkg.org) for the production, curation, and (re)use of this data.

The core entity in the ORKG is contributions presented in the form of research papers. A contribution is typically linked to a research field and problem, and its description includes several properties that are specific to the research field or problem.

Based on described papers and their contributions, contributions dealing with a specific research problem in scholarly literature can be compared in so-called comparisons. Comparisons are tabular representations of the properties of all compared contributions22


Research data and their descriptions have a very complex structure and semantics.

he definition of necessary types of expected answers is based on the results of evaluation campaigns of QALD34 and analysis of characteristic problems associated with the task of mapping natural language to formal queries presented in Cimiano and Minock35

The definition of the focus of a question makes the search for an answer more specific. Moldovan et al.36 defined the question focus as a word or sequence of words that indicate what information is being asked about in the question. Ferret et al.37 defined the question focus as ‚Äúa noun phrase that is likely to be present in the answer‚Äù consisting of a head noun and a list of its modifiers. For example, the question ‚ÄúWhat types of nanocarriers do have therapeutic effect?‚Äù has the focus on ‚Äútypes of nanocarriers‚Äù. According to Mikhailian et al.38 there are two types of question foci:

1.
Asking Point (AP), which is denoted explicitly, e.g., words ‚Äúresearch problems‚Äù in the question ‚ÄúWhat are the research problems Vernier Effect is related to?‚Äù.

2.
ExpectedAnswerType (EAT), is an implicit answer that can be inferred from the information provided by the question, e.g., answer type ‚Äúperson‚Äù is the EAT for the question ‚ÄúWho are the authors of the SOSA ontology?‚Äù.

<!-- Interesting framework for categorizing questions and answers -->

For our methodology, we modified the approach of Moldovan et al.36 by combining the question types, e.g., WHAT, WHO, WHICH, etc., corresponding to classes from the ORKG schema, e.g., Paper, Problem, etc., and the question patterns that define the expected answer (BOOLEAN, WHAT-WHO, WHAT-WHEN, WHICH-WHERE, WHICH-WHAT, and WHO-WHAT). For instance, the question ‚ÄúWho is the author of the most recent paper about insects?‚Äù has the pattern WHO-WHAT. 


<!-- This is simply a bad question, no good answer possible -->

Number of triple patterns In contrast to simple questions, the SPARQL query of complex questions consists of more than a single triple pattern18. As presented in Tables 3 and 4, the dataset contains both simple and complex questions with up to 14 triple patterns.


# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q118188004)  
 * [Wikidata](https://www.wikidata.org/wiki/Q118188004)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q118188004&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1038/S41598-023-33607-Z)  

# Reading dates
  
 * 2023-08-24
