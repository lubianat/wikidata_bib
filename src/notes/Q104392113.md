
Concept Wikiﬁcation for COVID-19
================================
  
  [@wikidata:Q104392113]  

# Highlights

In this paper, we address the problem of concept wikification for COVID-19, which is to automatically recognize mentions of concepts related to COVID-19 in text and resolve them into Wikipedia titles.

We also develop an end-to-end system for concept wikification for COVID-19. Prelim- inary experiments show very encouraging re- sults. Our dataset, code and pre-trained model are available at github.com/panlybero/ Covid19_wikification.

This necessitates developing a concept recognition and linking system that can au- tomatically tag mentions of these concepts in text and resolve them into Wikipedia titles. We call this task concept wikification for COVID-19. It is an extension of the classic wikification (Roth et al., 2014)

he concept wikifier has to perform both concept mention detection and linking in an end-to-end fashion, since there is no mention extraction algorithm for COVID-19 concepts yet.

We first develop an ap- proach to automatically discover concepts related to COVID-19 by exploring Wikipedia, starting from the Wikipedia page for COVID-19.

- We developed an approach to automatically identify 7,238 concepts related to COVID-19.

- We curated a large labeled dataset for training
concept wikifiers for these concepts. We made the dataset available to the public.
- We developed an end-to-end system for COVID-19 concept wikification. Preliminary results are encouraging.

We repeat the following steps until we find a sufficient number of concepts relevant to COVID-19. 

Step 1: Start with known relevant pages S about COVID-19: parse each page p ∈ S, and then find all concepts C mentioned in p.

Step 2: For each c ∈ C, fetch the title page p? for c and add p? into S.
In a nutshell, we iteratively grow the set of COVID-19 relevant concepts by a breadth-first- search (BFS), starting at the Wikipedia entry for COVID-19 1 and expanding outward by following links included in that page. We run BFS with a maximum depth of 2, to avoid exploding the search space. This results in a total of 11,795 concepts.

First, we remove very rare concepts from the dataset, as these mostly consist of outliers. Second, we remove concepts of types such as cities and countries, that are covered extensively in existing datasets. 2.
The concept types are extracted from WikiData: https: //www.wikidata.org/wiki.

Given an input sentence, it first identifies concept strings (e.g., SARS-CoV-2) that are unambiguous and assigns their types using a majority-class classifier. It then applies a SciB- ERT (Beltagy et al., 2019)-based neural wikifier to assign types to other, more ambiguous concept mention strings.

Figure 2: The neural SciBERT-based wikifier. It takes a sequence of words as input and outputs a sequence of labels corresponding to wikipedia titles or Other (O).

Since most of the concepts are from scientific literature, we use SciBERT (Beltagy et al., 2019), a BERT variant that is pretrained on 1.14M scientific papers.

Deep contextualized word representations need to be fine-tuned in order to work well for this challenging problem.

# Comments

Interesting project. Results are not super impressive, though.

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q104392113)  
 * [Wikidata](https://www.wikidata.org/wiki/Q104392113)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q104392113/P921%3BP4510)  
