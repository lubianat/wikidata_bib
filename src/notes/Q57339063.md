
Decentralized provenance-aware publishing with nanopublications
===============================================================
  
  [@wikidata:Q57339063]  

# Highlights
In particular, there exist currently no efficient, reliable, and agreed-upon methods for publishing scientific datasets, which have become increasingly important for science. In this article, we propose to design scientific data publishing as a web-based bottom-up process, without top-down control ofcentral authorities such as publishing companies. Based on a novel combination of existing concepts and technologies, we present a server network to decentrally store and archive data in the form of nanopublications, an RDF-based format to represent scientific data.

To address some of these problems, a number of scientific data repositories have appeared, such as Figshare and Dryad (http://figshare.com, http://datadryad.org). Furthermore, Digital Object Identifiers (DOI) have been advocated to be used not only for articles but also for scientific data (Paskin, 2005).

let us assume that we conduct an analysis using, say, 1,000 individual data entries from each of three very large datasets (containing, say, millions ofdata entries each). How can we now refer to exactly these 3,000 entries to justify whatever conclusion we draw from them? The best thing we can currently do is to republish these 3,000 data entries as a new dataset and to refer to the large datasets as their origin. Apart from the practical disadvantages of being forced to republish data just to refer to subsets of larger datasets, other scientists need to either (blindly) trust us or go through the tedious process of semi-automatically verifying each of these entries

While narrative articles would still have their place in the academic landscape, small formal data snippets in the form ofnanopublications should take their central position in scholarly communication (Mons et al., 2011).

the nanopublication approach boils down to the ideas of subdividing scientific results into atomic assertions, representing these assertions in RDF, attaching provenance information in RDF on the level of individual assertions, and treating each of these tiny entities as an individual publication

facts’’ in human- ities datasets (such as prosopographies) have often been called ‘‘factoids’’ (Bradley, 2005), as they have to account for a degree of uncertainty. Nanopublications, with their support for granular context and provenance descriptions, offer a novel paradigm for publishing such factoids.

--> I like this term.

A recent study found that more than half of the publicly accessible SPARQL endpoints are available less than 95% of the time (Buil-Aranda et al., 2013), posing a major problem to services depending on them

The approach that we present below is based on previous work, in which we proposed
trusty URIs to make nanopublications and their entire reference trees verifiable and immutable by the use of cryptographic hash values (Kuhn & Dumontier, 2014; Kuhn & Dumontier, 2015). This is an example of such a trusty URI: http://example.org/r1.RA5AbXdpz5DcaYXCh9l3eI9ruBosiL5XDU3rxBbBaUO70

Furthermore, we argued in previous work that the assertion of a nanopublication need
not be fully formalized, but we can allow for informal or underspecified assertions (Kuhn et al., 2013), to deal with the fact that the creation of accurate semantic representations can be too challenging or too time-consuming for many scenarios and types of users.

We proposed a controlled natural language (Kuhn, 2014) for these informal statements, which we called AIDA (standing for the introduced restriction on English sentences to be atomic, independent, declarative, and absolute), and we had shown before that controlled natural language can also serve in the fully formalized case as a user-friendly syntax for representing scientific facts (Kuhn et al., 2006).

--> All cool stuff. I'm going do dive in Kuhn's works.

A single URI that does not resolve or a single server that does not respond can break the entire process. We argue here that we need distributed and decentralized services to allow for robust and reliable applications that consume Linked Data

Specifically, a nanopublication server of the current network has the following components: 
• A key-value store of its nanopublications (with the artifact code from the trusty URI as the key)
• A list ofknown peers, i.e., the URLs of other nanopublication servers • Information about each known peer, including the journal identifier and the total number of nanopublications at the time it was last visited.

The current network of 15 server instances on 10 sites (in 8 countries) is shown in Fig. 5, which is a screenshot of a nanopublication monitor that we have implemented (https://github.com/tkuhn/nanopub-monitor). Such monitors regularly check the nanopublication server network, register changes (currently once per minute), and test the response times and the correct operation of the servers by requesting a random nanopublication and verifying the returned data.

We have presented here a low-level infrastructure for data sharing, which is just one piece of a bigger ecosystem to be established. The implementation of components that rely on this low-level data sharing infrastructure is ongoing and future work. This

# Comments

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q57339063)  
 * [Wikidata](https://www.wikidata.org/wiki/Q57339063)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q57339063/P921%3BP4510)  
