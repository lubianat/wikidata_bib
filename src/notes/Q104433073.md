
SciBERT: A Pretrained Language Model for Scientific Text
========================================================
  
  [@wikidata:Q104433073]  

# Highlights

Obtaining large-scale annotated data for NLP tasks in the scientific domain is challeng- ing and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve perfor- mance on downstream scientific NLP tasks

https://github.com/allenai/scibert/

--> Very nice repo.

(i) We release SCIBERT, a new resource demon-
strated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pre- trained language model based on BERT but trained on a large corpus of scientific text. 

(ii) We perform extensive experimentation toinvestigate the performance of finetuning versus task-specific architectures atop frozen embed- dings, and the effect of having an in-domain vo- cabulary.

We construct SCIVOCAB, a newWordPiece vo-
cabulary on our scientific corpus using the Sen- tencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The re- sulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial dif- ference in frequently used words between scientific and general domain texts

We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text.

Sci- Cite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers

Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models for all other tasks.

Recent work on domain adaptation of BERT in- cludes BIOBERT (Lee et al., 2019) and CLIN- ICALBERT (Alsentzer et al., 2019; Huang et al., 2019).

For future work, we will release a version of
SCIBERT analogous to BERT-Large, as well as ex- periment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource thatâ€™s useful across multiple domains.


# Comments
I  really do not understand that much the techniques used there.


# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q104433073)  
 * [Wikidata](https://www.wikidata.org/wiki/Q104433073)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q104433073/P921%3BP4510)  
