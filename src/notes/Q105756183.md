Bioinformatics Data Skills
==========================
  
  [@wikidata:Q105756183]  
  

# Highlights

## Chapter 2 

The  actual  process  is  quite  simple:  laying  out  a  project  only entails  creating  a  few  directories  with  mkdir  and  empty  README  files  with  touch(commands we’ll see in more depth later). But this simple initial planning pays off inthe long term.

In these cases, it’s important to always use relative paths (e.g., ../data/stats/qual.txt). Using absolute paths leaves your work less portable between collaborators and decreases reproducibility.

- Document your methods and workflows

This should include full command lines (copied and pasted) that are run throughthe  shell  that  generate  data  or  intermediate  results.

--> I never do that, shame on me

- Document the origin of all data in your project directory

You need to keep track of where data was downloaded from, who gave it to you,and  any  other  relevant  information. It’s  important  to  include  when  the  data  was  downloaded.

- Document the versions of the software that you ran

All of this information is best stored in plain-text README files. 
Where  should  you  keep  your  README  files?  A  good  approach  is  to  keep  READMEfiles in each of your project’s main directories.

Ultimately,  you’ll  arrive  at  your  own  project  organization  system  thatworks for you; the take-home point is: leverage directories to help stay organized.

### Shell Expansion Tips

 If you’ve ever typed cd ~ togo  to  your  home  directory,  you’ve  used  shell  expansion—it’s  yourshell  that  expands  the  tilde  character  (~)

 $ mkdir -p zmays-snps/{data/seqs,scripts,analysis}

 it’s best to be as restrictive as possible when using wildcards. Instead of zmaysB*,use zmaysB*fastq or zmaysB_R?.fastq (the ? only matches a single character).

 Another useful trick is to use leading zeros (e.g., file-0021.txt ratherthan  file-21.txt)  when  naming  files.

--> Malandragem

### Markdown for Project Notebooks

Plain text is alsoa   future-proof  format:  plain-text  files  written  in  the  1960s  are  still  readable  today,whereas files from word processors only 10 years old can be difficult or impossible toopen  and  edit. 

Pandoc can convert between a variety of different markup and output formats. UsingPandoc  is  very  simple—to  convert  from  Markdown  to  HTML,  use  the  --from markdown and --to html options and supply your input file as the last argument:$ pandoc --from markdown --to html notebook.md > output.html


## Chapter 3

--> Pergunta: Qual a primeira vez que vcs se sentiram impressionados com o poder do shell?

The  Unix  shell  provides  a  way  for  these  programs  to  talk  to  each  other  (pipes)  andwrite to and read files (redirection).
We’ll address Unix streams in this chapter, but the conceptof  a  stream  is  very  important  in  how  we  process  large  data.
If   I   needed   to   search   for   the   exact   string   “GTGAT‐TAACTGCGAA”  in  this  data,  I  couldn’t  open  up  a  lane  of  data  in  Notepad  and  usethe  Find  feature  to  pinpoint  where  it  occurs—there  simply  isn’t  enough  memory  tohold  all  these  nucleotides  in  memory.  Instead,  tools  must  rely  on  streams  of  data,being  read  from  a  source  and  actively  processed.

make sure you’re using the Bourne-again shell,  or  bash.

--> The z shell is cooler though https://en.wikipedia.org/wiki/Z_shell

In  mydaily  bioinformatics  work,  I  use  Z  shell  (zsh)

--> Noice

Unix  was  not  designed  to  stop  its  users  from  doing  stupid  things,  as  that  would  alsostop them from doing clever things.
—Doug Gwyn

--> This is a very generalizable principle. It is the basis of Wiki, in a sense. 

Additionally,  pasting  contents  into  a  file  doesn’t  follow  a  recommendation  fromChapter 1: treat data as read-only.

wecan  combine  large  files  by  printing  their  contents  to  the  standard  output  stream and redirect this stream from our terminal to the file we wish to save the combined resultsto.

We  use  the  operators  >  or  >>  to  redirect  standard  output  to  a  file.  The  operator  >redirects  standard  output  to  a  file  and  overwrites  any  existing  contents  of  the  file(take  note  of  this  and  be  careful),  whereas  the  latter  operator  >>  appends  to  the  file(keeping  the  contents  and  just  adding  to  the  end).

lrt  to  the  ls  lists  files  in  this  directory  in  list  format  (-l),  in  reverse  (-r)time  (-t)  order

Like standardoutput,  standard  error  is  by  default  directed  to  your  terminal.  In  practice,  we  oftenwant to redirect the standard error stream to a file so messages, errors, and warningsare logged to a file we can check later.

standard error stream, 2>

--> Cool, did not know that

Unix-like  operating  systems  have  a  special  “fake”  disk(known as a pseudodevice) to redirect unwanted output to: /dev/null. Output writtento  /dev/null  disappears,  which  is  why  it’s  sometimes  jokingly  referred  to  as  a  “black‐hole” by nerds.

It’s a bit more common to use Unix pipes (e.g., cat inputfile | program > outputfile) than <

We  use  pipes  in  bioinformatics  (quite  compulsively)not only because they are useful way of building pipelines, but because they’re faster(in  some  cases,  much  faster).

The Golden Rule of Bioinformatics is to not trust your tools or data. This skepticismrequires  constant  sanity  checking  of  intermediate  results,  which  ensures  your  meth‐ods  aren’t  biasing  your  data,  or  problems  in  your  data  aren’t  being  exacerbated  byyour methods

invert  the  matching  lines  with  the  grep  option  -v.

 Whenused in brackets, a caret symbol matches anything that’s not one of the charactersin these brackets. So the pattern [^ATCG] matches any character that’s not A, T, C,or  G.  Also,  we  ignore  case  with  -i,  because  a,  t,  c,  and  g  are  valid  nucleotides(lowercase characters are often used to indicate masked repeat or low-complexitysequences).  Finally,  we  add  grep’s  --color  option  to  color  the  matching  non-nucleotide characters

 --> cool

 Most bioinformaticians have made this mistake at somepoint and learned the hard way (by losing the FASTA file they were hoping to grep),so beware.

 --> Dang that sucks

 backslash  is  used  to  split  these  commandsacross multiple lines to improve readability (and is optional in your own work).Meanwhile,  program2  uses  the  standard  output  from  program1  as  its  standa

 --> I underuse that

 Theoperator is what redirects standard error to the standard output stream.

  Like  aplumber’s  T  joint,  the  Unix  program  tee  diverts  a  copy  of  your  pipeline’s  standardoutput  stream  to  an  intermediate  file  while  still  passing  it  through  its  standard  out‐put:$ program1 input.txt | tee intermediate-file.txt | program2 > results.txt

  --> Also super cool.

  We  can  tell  the  Unix  shell  to  run  a  program  in  the  background  by  appending  anampersand (&) to the end of our command

  --> rstudio . & saves lives

e can suspend processes by sending a stopsignal  through  the  key  combination  Control-z hen  use  the  bg  com‐mand  to  run  it  in  the  background. 

 If the process is currently runningin your shell, you can kill it by entering Control-C

y  Unix  standards,  an  exit  status  of  0  indicates  the  process  ran  successfully,  and  anynonzero status indicates some sort of error has occurred (and hopefully the programprints an understandable error message, too).

--> Learned that on CS50

The exit status isn’t printed to the terminal, but your shell will set its value to a vari‐able in your shell (aptly named a shell variable) named $?
completed successfully (&&), and one operator that runs the next command only if thefirst  completed  unsuccessfully  (||)


command  substitution.  Commandsubstitution runs a Unix command inline and returns the output as a string that canbe used in another command.

Using  this  command  substitution  approach,  we  can  easily  create  dated  directories
$ mkdir results-$(date +%F)$
$ ls results-2015-04-13

hen results are sorted by name, directories in this format also sort chrono‐logically

The cleverness behind this is what makes this date format, known as ISO 8601, useful.

A  word  of  warning,  though:  do  not  use  your  aliased  command  inproject-level  shell  scripts!  These  reside  in  your  shell’s  startup  file(e.g., ~/.profile or ~/.bashrc), which is outside of your project directory.



## Chapter 4
 We  use  SSH  because  it’s  encrypted(which makes it secure to send passwords, edit private files, etc.), and because it’s onevery  Unix  system.

The  material  covered  in  this  section  should  helpyou answer common SSH questions a sysadmin may ask (e.g., “Do you have an SSHpublic  key?”).  You’ll  also  learn  all  of  the  basics  you’ll  need  as  a  bioinformatician  toSSH into remote machines

--> Nice!

ssh biocluster.myuniversity.edu

SH also works with IP addresses—for example, you could connect to a machine withssh 192.169.237.42.  If  your  server  uses  a  different  port  than  the  default  (port  22),or  your  username  on  the  remote  machine  is  different  from  your  local  username,you’ll need to specify these details when connecting:$

 ssh -p 50453 cdarwin@biocluster.myuniversity.edu

the SSH configfile.  SSH  config  files  store  details  about  hosts  you  frequently  connect to.

 ~/.ssh/config.
 Eachentry takes the following form:
 Host bio_serv     
 HostName 192.168.237.42     
 User cdarwin     
 Port 50453

 You  won’t  need  to  specify  Port  and  User  unless  these  differ  fromthe  remote  host’s  defaults.
 With  this  file  saved,  you  can  SSH  into192.168.236.42 using the alias ssh bio_serv.

 --> Nooice

 $ hostname
 biocluster.myuniversity.edu

$ whoami
cdarwin

A safer, easier alternative is to use anSSH public key. Public key cryptography is a fascinating technology, but the details areoutside  the  scope  of  this  book. 

--> Love that he leaves the details out (no kidding).

enerate  a  public/private  key  pair.  We  do  thiswith  the  command  ssh-keygen.  It’s  very  important  that  you  note  the  differencebetween  your  public  and  private  keys:  you  can  distribute  your  public  key  to  otherservers, but your private key must be kept safe and secure and never shared

$ ssh-keygen -b 2048

This  creates  a  private  key  at  ~/.ssh/id_rsa  and  a  public  key  at  ~/.ssh/id_rsa.pub.

To  use  password-less  authentication  using  SSH  keys,  first  SSH  to  your  remote  hostand log in with your password. Change directories to ~/.ssh, and append the contentsof  your  public  key  file  (id_rsa.pub,  not  your  private  key!)  to  ~/.ssh/authorized_keys.

After  you’ve  added  your  public  key  to  the  remote  host,  try  logging  in  a  few  times.You’ll  notice  that  you  keep  getting  prompted  for  your  SSH  key’s  password.  If  you’rescratching your head wondering how this saves time, there’s one more trick to know:ssh-agent.  SSH agent is usually already running on Unix-based systems, but if not, youcan use eval ssh-agent to start it.

if our network connection tem‐porarily  drops  out.  This  behavior  is  intentional—your  program  will  receive  thehangup signal (referred to more technically as SIGHUP), which will in almost all casescause  your  application  to  exit  immediately.

nohup and Tmux. If you use a cluster, there are better ways to deal with hangups (e.g.,submitting  batch  jobs  to  your  cluster’s  software),  but  these  depend  on  your  specificcluster configuration. In this case, consult your system administrator

--> Who uses other systems, like HPC systems?

nohup program1 > output.txt & [1] 10900 We run the command with all options and arguments as we would normally, butby  adding  nohup  this  program  will  not  be  interrupted  if  your  terminal  were  toclose  or  the  remote  connection  were  to  drop.  Additionally,  it’s  a  good  idea  toredirect  standard  output  and  standard  error  just  as  we  did  in  Chapter  3  so  youcan check output later.

Tmux (and terminal multiplexers in general) allow you to create a session containingmultiple  windows,  each  capable  of  running  their  own  processes.  Tmux’s  sessions  arepersistent,  meaning  that  all  windows  and  their  processes  can  easily  be  restored  byreattaching the session.

I  strongly  suggest  you  go  to  this  chapter’s  directory  on  GitHub  anddownload the .tmux.conf file to your home directory.

--> Cool

tmux new-session -s maize-snps

--> I use this so much that I've aliased it to "tt"



## Chapter 5

you    may    have    files    with    names    such    as    thesis-vers1.docx,    thesis-vers3_CD_edits.docx,    analysis-vers6.R
However,  this  ad  hoc  file  versioning  systemdoesn’t scale well to complicated bioinformatics project

I highly recommend you take the timeto learn Git in this chapter, but be aware that understanding Git (like most topics inthis  book,  and  arguably  everything  in  life)  will  take  time  and  practice

Why learn‐ing Git is definitely worth the effort.

--> Nice that he took the time to outline that 

* Git Allows You to Keep Snapshots of Your Project

--> Narrative example is very useful

A good analogy comes from my friend and colleagueMike Covington: imagine you keep a lab notebook in pencil, and each time you run anew PCR you erase your past results and jot down the newest ones. This may soundextreme, but is functionally no different than changing code and not keeping a recordof past versions.

* Git Helps You Keep Track of Important Changes to Code
The  bioinformatician  quickly  emails  everyone  in  his  lab  the  new  ver‐sion and warns them of the potential for incorrect results. Unfortunately, members ofthe other lab may not get the message and could continue using the older buggy ver‐sion of the script.

git init creates a hidden directory called .git/ in your zmays-snps/ project directory(you  can  see  it  with  ls -a).  This  .git/  directory  is  how  Git  manages  your  repositorybehind  the  scenes.

--> Esse detalhe é crucial

Although you’ve initialized the zmays-snps/ as a Git repository, Git doesn’t automati‐cally begin tracking every file in this directory. Rather, you need to tell Git which filesto track using the subcommand git add. 

 This  extra  step  may  seem  like  an  inconvenience  but  actually  has  many  benefits.

 git commit: Taking a Snapshot of Your Project
 
 Optionally, you can omitthe  -m  option,  and  Git  will  open  up  your  default  text  editor.  If  you  prefer  to  writecommit  messages  in  a  text  editor  (useful  if  they  are  multiline  messages),  you  canchange the default editor Git uses with:
 $ git config --global core.editor emacs
 
 where  emacs  can  be  replaced  by  vim  (the  default)  or  another  text  editor  of  yourchoice.

 --> Relevant XKCD: https://xkcd.com/1296/

 --> Which resources help to write good commit messages? 
 --> https://chris.beams.io/posts/git-commit/
 --> https://dev.to/helderburato/patterns-for-writing-better-git-commit-messages-4ba0

   Another  subcom‐mand is quite helpful in this process: git diff.Without any arguments, git diff shows you the difference between the files in yourworking directory and what’s been staged.

   --> I use it less than I should

   git log  opens  up  your  repository’s  history  in  your  default  pager(usually either the program more or less). If you’re unfamiliar withpagers,  less,  and  more,  don’t  fret.  To  exit  and  get  back  to  yourprompt,  hit  the  letter  q.

     Tomove or remove tracked files in Git, we need to use Git’s version of mv and rm: git mvand git rm

  --> Wow, I did not know that.

  These  should  be  ignored  and  managed  by  other  means,  as  Git  isn’t  designed  tomanage really large files.

  --> dvc.org is a super good work around

  ext editor temporary filesText  editors  like  Emacs  and  Vim  will  sometimes  create  temporary  files  in  yourdirectory.  These  can  look  like  textfile.txt~  or  #textfile.txt#.  There’s  no  point  instoring  these  in  Git,  and  they  can  be  an  annoyance  when  viewing  progress  withgit status.  These  files  should  always  be  added  to  .gitignore.  Luckily,  .gitignoretakes wildcards, so these can be ignored with entries like *~ and \#*\#

  --> Cool

Merge conflicts seem scary, but the strategy to solve them isalways the same:
1.Use git status to find the conflicting file(s).
2.Open and edit those files manually to a version that fixes the conflict.
3.Use git add to tell Git that you’ve resolved the conflict in a particular file.
4.Once  all  conflicts  are  resolved,  use  git  status  to  check  that  all  changes  arestaged

By default, git checkout restores the file version from HEAD. However, git checkoutcan restore any arbitrary version from commit history.

One  very  useful  Git  subcommand  is  git stash,  which  saves  any  working  changesyou’ve  made  since  the  last  commit  and  restores  your  repository  to  the  version  atHEAD.  You  can  then  reapply  these  saved  changes  later.  git stash  is  handy  when  wewant  to  save  our  messy,  partial  progress  before  operations  that  are  best  performedwith  a  clean  working  directory—for  example,  git  pull  or  branching  (more  onbranching later).

--> I almost never use that

One  use  for  git diff  is  to  compare  the  difference  between  two  arbitrary  commits.

The  caret  notation  (^)  represents  the  parent  commit  of  a  commit.  For  example,  torefer to the parent of the most recent commit on the current branch (HEAD), we’d useHEAD^. HEAD^^ is the same as HEAD~2

--> Cool, I did not know that

 Git also has a tool called git bisect to help devel‐opers find where exactly bugs entered their commit history. git bisect is out of thescope of this chapter, but there are some good examples in git bisect --help

 git commit --amend opens up your last commit message in your default text editor,allowing you to edit it. 

 git fetch doesn’t change any of your local branches; rather, it just synchronizes yourremote branches with the newest commits from the remote repositories. 

  After  you’ve  mastered  all  of  these  concepts,  youmay  want  to  move  on  to  more  advanced  Git  topics  such  as  rebasing  (git rebase),searching  revisions  (git grep),  and  submodules.  However,  none  of  these  topics  arerequired in daily Git use; you can search out and learn these topics as you need them.A  great  resource  for  these  advanced  topics  is  Scott  Chacon  and  Ben  Straub’s  Pro  Gitbook.

  --> Hmm, seem useful, though


## Chapter 6

Two common command-line programs for downloading data from the Web are wgetand curl.

--> Quais outras formas vcs usam? Alguém usa algo direto do R ou do python?
 
wget  is  useful  for  quickly  downloading  a  file  from  the  command  line—for  example,human chromosome 22 from the GRCh37 (also known as hg19) assembly version:$ wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/chr22.fa.gz

For  simple  HTTP  or  FTP  authentica‐tion,  you  can  authenticate  using  wget’s  --user=  and  --ask-password  options.

One  of  wget’s  strengths  is  that  it  can  download  data  recursively.  When  run  with  therecursive  option  (--recursive  or  -r),  wget  will  also  follow  and  download  the  pageslinked to, and even follow and download links on these pages, and so forth.

--> Noice

Exercise  caution  when  using  wget’s  recursive  option;

curl serves a slightly different purpose than wget. wget is great for downloading filesvia HTTP or FTP and scraping data from a web page using its recursive option. curlbehaves similarly, although by default writes the file to standard output. To downloadchromosome 22 as we did with wget, we’d use:$ curl http://[...]/goldenPath/hg19/chromosomes/chr22.fa.gz > chr22.fa.gz

 A better tool for synchronizing these entire directories acrossa network is Rsync.Rsync  is  a  superior  option  for  these  types  of  tasks  for  a  few  reasons.  First,  Rsync  isoften  faster  because  it  only  sends  the  difference  between  file  versions  (when  a  copyalready  exists  or  partially  exists) 

 rsync’s basic syntax is rsync source destination,

 The  option  -a  enables  wrsync’s  archive  mode,  -z  enablesfile transfer compression, and -v makes rsync’s progress more verbose so you can seewhat’s being transferred. Because we’ll be connecting to the remote host through SSH,we also need to use -e ssh. Our directory copying command would look as follows:$ rsync -avz -e ssh zea_mays/data/ vinceb@[...]:/home/deborah/zea_mays/data

 --> So much cool stuff.

  Checksums  are  very  compressed  summaries  of  data,  computed  in  a  way  thateven if just one bit of the data is changed, the checksum will be different.The  checksums  would  differ  if  thedata  changed  even  the  tiniest  bit,  so  we  can  use  them  to  calculate  the  version  of  thedata.

  et’s  get  acquainted  with  checksums  using  SHA-1.  We  can  pass  arbitrary  strings  tothe program shasum (on some systems, it’s sha1sum) through standard in:$ echo "bioinformatics is fun" | shasumf9b70d0d1b0a55263f1b012adab6abf572e3030b  -$ echo "bioinformatic is fun" | shasume7f33eedcfdc9aef8a9b4fec07e58f0cf292aa67  -

  --> Nice example

  Then,  we  can  use  shasum’s  check  option  (-c)  to  validate  that  these  files  match  theoriginal versions

  --> Alguém faz isso de forma regular? eu _nunca_ faço

For  the  most  part,  data  can  remain  compressed  on  the  disk  throughout  processingand  analyses.  Most  well-written  bioinformatics  tools  can  work  natively  with  com‐pressed data as input, without requiring us to decompress it to disk first.

trimmer in.fastq.gz | gzip > out.fastq.gzgzip takes input from standard in, compresses it, and writes this compressed outputto standard out.

f  programs  cannot  handle  compressed  input,  you  can  use  zcat  and  pipe  outputdirectly to the standard input of another program.These  programs  that  handle  compressed  input  behave  exactly  like  their  standardcounterpart. For example, all options available in grep are available in zgrep:$ 

zgrep --color -i -n "AGATAGAT" Csyrichta_TAGGACT_L008_R1_001.fastq.gz2706: ACTTCGGAGAGCCCATATATACACACTAAGATAGATAGCGTTAGCTAATGTAGATAGATT

--> nice nice nice


And  again,  we  copy  the  SHA-1  into  our  README.md.  So  far,  our  README.mdmight look as follows:## Genome and Annotation DataMouse (*Mus musculus*) reference genome version GRCm38 (Ensemblrelease 74) was downloaded on Sat Feb 22 21:24:42 PST 2014, using:    wget ftp://ftp.ensembl.org/[...]/Mus_musculus.GRCm38.74.dna.toplevel.fa.gzGene annotation data (also Ensembl release 74) was downloaded from Ensembl onSat Feb 22 23:30:27 PST 2014, using:    wget ftp://ftp.ensembl.org/[...]/Mus_musculus.GRCm38.74.gtf.gz## SHA-1 Sums - `Mus_musculus.GRCm38.74.dna.toplevel.fa.gz`: 01c868e22a9815c[...]c2154c20ae7899c5f - `Mus_musculus.GRCm38.74.gtf.gz`: cf5bb5f8bda2803[...]708bff59cb575e379Although this isn’t a lot of documentation, this is infinitely better than not document‐ing how data was acquired.


--> Nice nice nice, too often people (including myself) forget to document how a piece of data was obtained.

## Chapter 7

We  often  forget  how  science  and  engineering  function.  Ideas  come  from  previousexploration more often than from lightning strokes.—John W. Tukey

ount and print the k most common words in a file 

literateprogramming,  a  method  of  programming  that  Knuth  pioneered.  Literate  programsare  written  as  a  text  document  explaining  how  to  solve  a  programming  problem  (inplain  English)  with  code  interspersed  throughout  the  document.  Code  inside  thisdocument  can  then  be  “tangled”  out  of  the  document  using  literate  programmingtools  (this  approach  might  be  recognizable  to  readers  familiar  with  R’s  knitr  orSweave—both are modern descendants of this concept).

McIlroy replied with a six-line Unix script that solved the same programming problem:tr 
-cs A-Za-z '\n' | tr A-Z a-z | sort | uniq -c | sort -rn | sed ${1}q

 Also, his solution was built on reusable Unix data tools (or as he called them,“Unix  staples”)  rather  than  “programmed  monolithically  from  scratch,”  to  use  McIl‐roy’s phrasing

 Many  tasks  in  bioinformatics  are  of  this  nature:  we  want  to  get  a  quick  answer  andkeep moving forward with our project. We could write a custom script, but for simpletasks  this  might  be  overkill  and  would  take  more  time  than  necessary.  

 --> That is also the value of organized linked open data.

We can also use tail to remove the header of a file. Normally the -n argument speci‐fies how many of the last lines of a file to include, but if -n is given a number x pre‐ceded with a + sign (e.g., +x), tail will start from the xth line.


 inspect the first and last 3 lines of a file
i() { (head -n 2; tail -n 2) < "$1" | column -t}

--> Neat trick

After printing the first few rows of your data to ensure your pipeline is working prop‐erly, the head process exits. This is an important feature that helps ensure your pipesdon’t  needlessly  keep  processing  data.  When  head  exits,  your  shell  catches  this  andstops the entire pipe, including the grep process too. Under the hood, your shell sendsa signal to other programs in the pipe called SIGPIPE—much like the signal that’s sentwhen  you  press  Control-c  (that  signal  is  SIGINT).

--> cool

less  runs  more  like  an  application  than  a  command:  once  we  start  less,  it  will  stayopen until we quit it
First,  if  you  need  to  quit  less,  press  q.

--> Nice less tricks

One  of  thegreat  beauties  of  the  Unix  pipe  is  that  it’s  easy  to  debug  at  any  point—just  pipe  theoutput of the command you want to debug to less and delete everything after. Whenyou run the pipe, less will capture the output of the last command and pause so youcan inspect it

Theresult  is  that  we  can  throw  less  after  a  complex  pipe  processing  large  data  and  notworry  about  wasting  computing  power—the  pipe  will  block  and  we  can  spend  asmuch time as needed to inspect the output.

--> noice

With  plain-text  data  formats  like  tab-delimited  and  CSV  files,  the  number  ofrows  is  usually  the  number  of  lines.  We  can  retrieve  this  with  the  program  wc  (forword count)
wc outputs the number of words, lines, and characters
Often,  we  only  care  about  the  number  of  lines.  We  can  use  option  -l  to  just  returnthe number of lines

ls, with the -l option for size,  If we wish to use human-readable sizes, we can use ls -lh:$ ls -lh

--> Cool

Tosee how many columns of data there are, we need to first chop off the comments andthen pass the results to our awk one-liner.
tail -n +6 Mus_musculus.GRCm38.75_chr1.gtf | awk -F "\t" '{print NF; exit}'

--> That is good and fast. Loading huge tables into memory in R just for counting columns seems like a waste of resources


suppose we wanted to extract only the start positions (the second column) of theMus_musculus.GRCm38.75_chr1.bed  file.  The  simplest  way  to  do  this  is  with  cut

cut treats tabs as the delimiters, so to extract the second column we use:$ cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3

$ grep -v "^#" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1-8 | head -n31       pseudogene      gene    3054233 3054733 .       +       .1       unprocessed_pseudogene  transcript      3054233 3054733 .       +       .1       unprocessed_pseudogene  exon    3054233 3054733 .       +       .

While tabs are a terrific delimiter in plain-text data files, our variable width data leadsour  columns  to  not  stack  up  well.

column -t produces neat columnsthat are much easier to read

First,  it’s  important  to  mention  grep  is  fast.  Really  fast.

$ grep "Olfr418-ps1" Mus_musculus.GRCm38.75_chr1_genes.txtENSMUSG00000049605      Olfr418-ps1The  quotes  around  the  pattern  aren’t  required,  but  it’s  safest  to  use  quotes  so  ourshells  won’t  try  to  interpret  any  symbols.

while we wanted to exclude “Olfr1413,” this command would also exclude genes like“Olfr1413a” and “Olfr14130.” But we can get around this by using -w, which matchesentire words (surrounded by whitespace)

rep’s default output often doesn’t give us enough context of a match when we needto inspect results by eye; only the matching line is printed to standard output. Thereare three useful options to get around this context before (-B), context: after (-A), andcontext before and after (-C)

grep  also  supports  a  flavor  of  regular  expression  called  POSIX  Basic  Regular  Expres‐sions  (BRE)

grep  has  an  option  to  count  how  many  lines  match  a  pattern:  -c.  For  example,  sup‐pose we wanted a quick look at how many genes start with “Olfr”:$ grep -c "\tOlfr" Mus_musculus.GRCm38.75_chr1_genes.txt

the  2%  of  the  time  when  encoding  does  matter—usuallywhen an invisible non-ASCII character has entered data—it can lead to major head‐aches.  In  this  section,  we’ll  cover  the  basics  of  inspecting  text  data  at  a  low  level  tosolve these types of problem

### Text Processing with Awk

 The key to using Awk effec‐tively is to reserve it for the subset of tasks it’s best at: quick data-processing tasks ontabular  data.  Learning  Awk  also  prepares  us  to  learn  bioawk,  which  we’ll  cover  in“Bioawk: An Awk for Biological Formats” on page 163.

 Awk was designed to workwith  tabular  data,  each  record  is  a  line,  and  each  field  is  a  column’s  entry  for  thatrecord. The clever part about Awk is that it automatically assigns the entire record tothe variable $0, and field one’s value is assigned to $1, field two’s value is assigned to$2, field three’s value is assigned to $3, and so forth.

 we  can  simply  mimic  cat  by  omitting  a  pattern  and  printing  an  entire  recordwith the variable $0:$ awk '{ print $0 }' example.bed

### sed

 he streameditor,  or  sed,  allows  you  to  do  exactly  that.  sed  is  remarkably  powerful,  and  hascapabilities  that  overlap  other  Unix  tools  like  grep  and  awk.  As  with  awk,  it’s  best  tokeep your sed commands simple at first. We’ll cover a small subset of sed’s vast func‐tionality that’s most useful for day-to-day bioinformatics tasks.

 As  withgrep,  we  can  use  the  -E  option  to  enable  POSIX  Extended  Regular  Expressions(ERE)

$ head -n 1 chroms.txt  # before sed
chrom1  3214482 3216968
$ sed 's/chrom/chr/' chroms.txt | head -n 1
chr1    3214482 3216968

--> Good example

^\(chr[^:]+\):. This matches the text that begins at the start ofthe line (the anchor ^ enforces this), and then captures everything between \( and \)

$ echo "chr1:28427874-28425431" | sed 's/[:-]/\t/g'chr1    28427874        2842543

Rather  than  explicitly  capturing  each  chunk  of  information  (the  chromosome,start  position,  and  end  position),  here  we  just  replace  both  delimiters  (:  and  -)with  a  tab.

--> cool

First, disable sed from outputting alllines with -n. Then, by appending p after the last slash sed will print all lines it’s madea replacement on. The following is an illustration of -n used with p:

sed -E -n 's/.*transcript_id "([^"]+)".*/\1/p'
## Chapter 8 
xploratory data analysis is an attitude, a state of flexibility, a willingness to look forthose things that we believe are not there, as well as for those we believe might be there.Except for its emphasis on graphs, its tools are secondary to its purpose

In  his  1977  book  Exploratory  Data  Analysis,  Tukeydescribed  EDA  as  “detective  work”  involved  in  “finding  and  revealing  the  clues”  indata.

As stated in Chapter 1,no  amount  of  post-experiment  data  analysis  can  rescue  a  poorly  designed  experi‐ment.  Likewise,  no  amount  of  terrific  exploratory  data  analysis  is  substitute  for  hav‐ing  a  good  experimental  question  and  applying  appropriate  statistical  methods.

This  downloads  and  installs  ggplot2  from  a  CRAN  repository  (for  information  onsetting various options, including how to configure your repository to a nearby mir‐ror, see help(install.packages)).

--> Lol, never ran that

Many of the difficul‐ties  beginning  R  users  face  stem  from  misunderstanding  the  language’s  basic  con‐cepts,  data  structures,  and  behavior  (which  can  differ  quite  significantly  from  otherlanguages like Python and Perl). In this section, we’ll learn how to do simple calcula‐tions in R, assign values to variables, and call functions. Then, we’ll look at R’s vectors,vector  data  types,  and  vectorization

You’ll  need  to  familiarize  yourself  with  some  R  lingo:  we  say  each  line  contains  anexpression that is evaluated by R when you press Enter


Behind the scenes, R uses print() to format the R objects you see printed as output.(note that you won’t see print() explicitly called).Alternatively,  you  can  change  the  default  number  of  significant  digits  R  uses  bychanging  the  global  option  in  R.  You  can  view  the  current  default  value  of  an  optionby  calling  getOption()  with  the  option  name  as  an  argument;  for  example,  you  canretrieve the number of significant digits printed as follows:> getOption('digits')[1] 7A new option value can be set using the function options():> options(digits=9)options()  contains  numerous  user-customizable  global  options.  See  help(options)for more information

--> Voces personalizam suas "global options" no R? Isso é uma boa prática? 

You  access  access  R’s  built-in  documentation  with  the  help()  func‐tion or its syntactic shortcut, 

help(log)
?log

Unfortunately, we often only have a fuzzier idea of what weneed  help  with  (e.g.,  what  was  the  function  in  R  that  calculates  cross  tabulate  vec‐tors?).   For   tasks   like   this,   we   can   search   R’s   help   system   with   the   function
help.search(), or its shortcut ?
?:> help.search("cross tabulate")> ??"cross tabulate"

--> Caraca, eu não sabia disso. Tipo, usava sem entender. 

Finally,   R   also   has   functions   for   listing   all   functions   in   a   package   (e.g.,library(help="base")) and finding functions by name (e.g., apropos(norm)), whichare often useful in remembering a function’s name

To store a value for future use, we assign it to a variable (also known as a symbol in Rjargon) using the <- assignment operator:> x <- 3.1

RStudio Assignment Operator ShortcutIn RStudio, you can create the <- assignment operator in one key‐stroke using Option - (that’s a dash) on OS X or Alt - on Windows/Linux.

--> Bom macete. Quais outros macetes de RStudio vcs usam?

R does not have a type for a sin‐gle value (known as a scalar) such as 3.1 or “AGCTACGACT.” Rather, these values arestored in a vector of length 1.

There’s one important subtle behavior of vectorized operations applied to two vectorssimultaneously: if one vector is longer than the other, R will recycle the values in theshorter  vector.

Be aware that R does not issue a warning if you try to access an ele‐ment  in  a  position  that’s  greater  than  the  number  of  elements—instead,  R  will  return  a  missing  value  (NA;  more  on  this  later)

 Unlike Python’s lists or Perl’s arrays, R’s vectors must contain elements ofthe same type

 you  can  explicitlytell R to treat a value as an integer by appending a capital L after the valu


T  and  F  are  assigned  the  values  TRUE  and  FALSE,  and  while  you  might  betempted  to  use  these  shortcuts,  do  not.  Unlike  TRUE  and  FALSE,  T  and  F  can  beredefined in code. Defining T <- 0 will surely cause problems

--> O biioconductor até avisa

NA  is  R’s  built-in  value  to  represent  missing  dat
NULL  represents  not  having  a  value  (which  is  different  than  having  a  value  that’smissing). It’s analogous to Python’s None value
NaN stands for “not a number,”


If these solutions are still tooslow,  you  can  install  the  data.table  package  and  use  its  fread()  function,  which  isthe  fastest  alternative  to  read.*  functions  (though  be  warned:  fread()  returns  adata.table, not a data.frame, which behaves differently; see the manual).

--> I usually do data.frame(fread("example.csv"))

A good solution for moderately large data is to use SQLite and query out sub‐sets  for  computation  using  the  R  package  RSQLite  (we’ll  cover  SQLite  and  otherstrategies for data too large to fit in memory in Chapter 13).

--> Nice

’s important to know that R does this so you can disable this coer‐cion  when  you  need  a  column  as  a  character  vector.  To  do  this,  set  the  argumentstringsAsFactors=FALSE  (or  use  asis;  see  help(read.table)  for  more  informa‐tion)

Getting Data into ShapeQuite  often,  data  we  load  in  to  R  will  be  in  the  wrong  shape  for  what  we  want  to  dowith  it.  Tabular  data  can  come  in  two  different  formats:  long  and  wide.  With  widedata, each measured variable has its own column (Table 8-5).

--> Nice


It’s a good idea to avoid referring to specific dataframe rows in youranalysis  code.  This  would  produce  code  fragile  to  row  permuta‐tions  or  new  rows  that  may  be  generated  by  rerunning  a  previousanalysis  step.  In  every  case  in  which  you  might  need  to  refer  to  aspecific row, it’s avoidable by using subsetting (see “Exploring DataThrough Slicing and Dicing: Subsetting Dataframes” on page 203).Similarly,  it’s  a  good  idea  to  refer  to  columns  by  their  columnname,  __not__  their  position.

Instead of spending time making your graph look pretty, [ggplot2 allows you to] focuson creating a graph that best reveals the messages in your data.—ggplot2:  Elegant  Graphics  for  DataAnalysis Hadley Wickham

he best up-to-datereference for ggplot2 is the ggplot2 online documentation. As you shift from begin‐ning ggplot2 to an intermediate user, I highly recommend the books ggplot2: ElegantGraphics  for  Data  Analysis  by  Hadley  Wickham  (Springer,  2010)  and  R  GraphicsCookbook by Winston Chang (O’Reilly, 2012) for more detail. 

As you can see from Figure 8-2, the region with missing diversity estimates is aroundthe  centromere.  This  is  intentional;  centromeric  and  heterochromatic  regions  wereexcluded from this study.

. As bin widthsbecome narrower, each bin will contain fewer data points and con‐sequently  be  more  noisy  (and  undersmoothed).  Using  wider  binssmooth  over  this  noise.  However,  bins  that  are  too  wide  result  inoversmoothing, which can hide details about the data. This trade-offis a case of the more general bias-variance trade-off in statistics; seethe Wikipedia pages on the bias–variance trade-off and histogramsfor more information on these topics. In your own data, be sure toexplore a variety of bin widths


Versions of R and any R packages installed change over time. This can lead to repro‐ducibility  headaches,  as  the  results  of  your  analyses  may  change  with  the  changingversion of R and R packages. Solving these issues is an area of ongoing development(see,  for  example,  the  packrat  package).  At  the  very  least,  you  should  always  recordthe  versions  of  R  and  any  packages  you  use  for  an  analysis.  R  actually  makes  thisincredibly easy to do—just call the sessionInfo()

--> Nice


# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q105756183)  
 * [Wikidata](https://www.wikidata.org/wiki/Q105756183)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q105756183/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q105756183&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+workhttps://tabernacle.toolforge.org/?#/tab/manual/Q105756183/P921%3BP4510)  
