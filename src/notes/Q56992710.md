
Citizen Science for Mining the Biomedical Literature
====================================================
  
  [@wikidata:Q56992710]  
  
Publication date : 31 of December, 2016  

# Highlights
We have previously demonstrated that named entity recognition (NER) tasks can be crowdsourced to a group of non-experts via the paid microtask platform, Amazon Mechanical Turk (AMT), and can dra-matically reduce the cost and increase the throughput of biocuration efforts. However, given the size of the biomedical literature, even information extraction via paid microtask platforms is not scalable. With our web-based application Mark2Cure (http://mark2cure.org), we demonstrate that NER tasks also can be performed by volunteer citizen scientists with high accuracy.

 This  study  and  the  subsequent  survey  was  reviewed  by  and  approved  by  the  Scripps  Health  Insti-tutional  Review  Board  and  placed  in  the  “exempt”  risk  category
 
 For  the  sake of usability, users who started a quest were allowed to finish it even if the quest was subsequently completed by the community. Hence it is possible for abstracts to be completed by more than 15 different users.

Beyond  the  training  modules,  Mark2Cure  participants  were  paired  with  other  users  and  given  points  based  on  their  performance  to  encourage  continuous  learning/improvement.  While  this  was  effective  when  users  were  paired  with  the  gold  standard  “expert  user”,  many  users  expressed frustration when paired with poor performing partners.  Issues with partner pairing highlighted the need to apply sorting mechanisms or allow for “expert trailblaz-ers” like those used in Eyewire.

Sessions  from  new  users  peaked  the  day  after  the  arti-cle on Mark2Cure was published in the San Diego Union Tribune  (Figure  1).

One  participant  from  this  community  approached  the  Missouri  Military  Academy  (MMA)  and  recruited  both  instructors  and  students  to  participate.  Over  24  participants  from  MMA  contributed  about 10% of the task completions

Overall,  the  accuracy  of  contributions  relative  to  the  gold standard was quite high. The average F-score per user across all of their annotations was 0.761 with a standard deviation of 0.143 (Figure 3B), which was on par with that of our previous AMT results (Good et al. 2015)


Table 1: Computed metrics of “Contributions to Science” and “Public Engagement” as defined by the Zooniverse pro-jects (Cox et al. 2015). As a single citizen science project, we cannot perform the internal comparisons used in the Zooniverse  paper  and  thus  have  no  basis  for  comparing  our  results  (Zooniverse  results  are  normalized).

The Distribution of Effort was higher than Zooniverse’s across project average of  0.18  (Simmons  2015)  but  similar  to  the  Andromeda  Project; this higher Distribution of Effort score may be an artifact of the short project period. 

--> Nice measures for citizen science.

).  Following  Galaxy Zoo’s example (Richards and Lintott 2012), we can set  up  computational  systems  that  learn  to  perform  the  current tasks of the citizen scientists. Once these methods reach acceptable levels of performance, the citizens can be directed toward other areas still in need of human input.


Although  we  currently  apply  Mark2Cure  to  create  an  annotated,  NGLY1-deficiency-specific  corpora,  we are also developing Mark2Cure toward more challeng-ing  areas  of  information  extraction  such  as  relationship  extraction.  We  expect  extracting  relationship  informa-tion in NGLY1-related literature to provide more insight-ful information and to be of greater utility to the NGLY1 researchers,  ultimately  leading  to  new  discoveries  in  this  field.Apart from helping to develop computational methods for  information  extraction,  citizen  science  participants  can  help  at  much  higher  levels  than  machines  are  likely  to reach. A motivated community of citizen scientists can accomplish  nearly  any  goal,  including  the  development  of their own computational methods for solving complex tasks  (Khatib  2011)

Furthermore, we demonstrate how researchers might utilize site traffic information and logged  data  to  improve  aspects  of  the  design  to  achieve  quality data, and we analyze our project with accordance to Cox et al.’s “Elements of citizen science success matrix” providing  a  comparison  point  for  other  citizen  science  efforts. 
# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q56992710)  
 * [Wikidata](https://www.wikidata.org/wiki/Q56992710)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q56992710/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q56992710&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.5334/CSTP.56)  
