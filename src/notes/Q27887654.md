
Crowdsourcing biomedical research: leveraging communities as innovation engines
===============================================================================
  
  [@wikidata:Q27887654]  
  
Publication date : 01 of July, 2016  

# Highlights

. Crowdsourcing the analysis of complex and massive data has emerged as a framework to find robust methodologies. When the crowdsourcing is done in the form of collaborative scientific competitions, known as Challenges, the validation of the methods is inherently addressed. Challenges also encourage open innovation, create collaborative communities to solve diverse and important biomedical problems, and foster the creation and dissemination of well-curated data repositories.

An emerging paradigm that brings together large numbers of research scientists to address complex prob­lems is the concept of crowdsourcing: a methodology that uses the voluntary help of large communities to solve problems posed by an organization

A Challenge is a specific form of crowdsourcing that is now very popular among research scientists. These Challenges can be competitions organized by academic groups or by a for­profit company; they use voluntary labour to solve their own problems or those of a third party (typically other for­profit companies)

The success of the crowdsourcing paradigm has spurred a proliferation of Challenge initiatives and plat­forms. Wikipedia17 lists more than 150 crowdsourcing projects  in  very  diverse  areas,  such  as  design  and   technology  innovation.

--> Cool Wikipedia mention

The for­profit side of crowd­sourcing Challenges is best exemplified by companies such as InnoCentive, Kaggle and Topcoder (FIG. 1)


Steps and components of a Challenge

The scientific question. Challenges often arise from sci­entific problems for which answers need new method development  and  validation28,  or  from  the  need  to  benchmark algorithms that yield divergent results and for which an objective evaluation could be appropri­ate29

Organizational infrastructure. Running Challenges requires input and expertise from different sets of spe­cialists who all need to work together in a coordinated fashion. I

Crowdsourcing data annotation and curation in bioinformatics can be handled well with this approach. This scheme has also been applied to provide pathway resources68,69, reconstruct the human metabolic network70, annotate molecular interactions in Mycobacterium tuberculosis71 and identify crucial errors in ontologies72.


Box 1 |Types of crowdsourcingGenerally speaking, crowdsourcing can refer to efforts in which the crowd provides data (for example, patients provide their medical information) to be mined by others or, alternatively, to initiatives in which the crowd actively works on solving a problem66. One type of active crowdsourcing is labour-focused crowdsourcing, in which work that needs to be done is proposed to a community willing to take up such a job13. A well-known example of labour-focused crowdsourcing is the ‘Mechanical Turk’ run by Amazon. The Mechanical Turk approach provides an online workforce that allows people to complete work, or ‘human intelligence tasks’, in exchange for a small amount of money67.A complex problem can be divided into a set of smaller, independent tasks to benefit from crowdsourcing. Crowdsourcing data annotation and curation in bioinformatics can be handled well with this approach. This scheme has also been applied to provide pathway resources68,69, reconstruct the human metabolic network70, annotate molecular interactions in Mycobacterium tuberculosis71 and identify crucial errors in ontologies72.In contrast to labour-focused forms of crowdsourcing, there are forms of crowdsourcing in which individuals participate because of their interest in the project or cause13. An example of this is the crowdsourced approach taken to develop the popular community encyclopedia Wikipedia. In some instances (such as Wikipedia and the protein structure game Foldit73), participants contribute their time and intellectual capacity, whereas in other examples (such as the Folding@home74 and Rosetta@home75 protein folding projects), participants provide computational power from their personal equipment to help solve the problem.In some instances, crowdsourcing can be implemented in the form of a game76 to maximize the number of solvers who work on the problem and to increase the likelihood that they will stay engaged. For example, in the Foldit project, the problem of determining protein structure is transformed into an entertaining game. Such ‘gamification’, in which game-design elements are used to allow an enjoyable experience, has proved a spectacular approach to raise participant numbers and interest. It also leads to results: Foldit’s 57,000 players provided useful results that matched or outperformed algorithmically computed solutions73. Foldit was followed by a similarly popular project, EteRNA77, in which more than 26,000 participants provided an RNA sequence that fits a given shape. The best designs, as chosen by the community, were then tested experimentally73,78. Hence, gamification is a powerful tool to engage massive numbers of volunteer citizen scientists to solve complex problems in which human intuition can outperform computer algorithms, even for abstract problems such as quantum computing79

--> Cool box, kind of relevant for 3.2.2. Community curation and gamified science

A major consideration in using the Challenge framework is the question of how best to incentivize participation. The most typical incentives are monetary awards, the possibility to co-author a high-profile paper reporting on a Challenge, an invitation to present the best-performing method at a conference or the desire to access and analyse the data sets provided in the Challenge.• Given that meaningful participation requires a substantial time investment from each team, a ‘winner-takes-all’ approach for selecting top performers can limit the diversity and depth of involvement, whereas intermediate awards can directly motivate participants to exert costly effort.• Unsportsmanlike behaviour — in which participants register under different identities in order to send more predictions to the leaderboard than allowed — has been observed, but fortunately this is rare and not difficult to detect



# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q27887654)  
 * [Wikidata](https://www.wikidata.org/wiki/Q27887654)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q27887654/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q27887654&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
