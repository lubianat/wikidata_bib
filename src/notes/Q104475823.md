
Bridging the Gap Between Research and Practice: Predicting What Will Work Locally
=================================================================================
  
  [@wikidata:Q104475823]  

# Highlights

Currently, research in evidence-based education policy and practice focuses on randomized controlled trials. These can support causal ascriptions (‘‘It worked’’) but provide little basis for local effectiveness predictions (‘‘It will work here’’), which are what matter for practice. We argue that moving from ascription to prediction by way of causal general- ization (‘‘It works’’) is unrealistic and urge focusing research efforts directly on how to build local effectiveness predictions.

Our principal contribution on the knowl- edge production side is in diagnosing the gap and showing that bridging it requires dramatically expanding the kinds of evidence that are collected and disseminated by EBE and adjusting the methods used to judge its acceptability.

EBE aims to support predictions concerning how an evidence-based intervention will perform in a new educational setting indirectly by establishing effectiveness generalizations. While gener- alizations would naturally justify predictions about specific cases, we argue that they are unnecessary.

If the program is to work in a context it will have to be fitted to that context, and this seldom involves just tinkering around the edges or implementing it well. Rather, it requires getting all the right fea- tures in place that will allow the program to work here and guarding against features that can derail it here.

RCTs are poor evidence for what educators need to know: Will this intervention work here in our school or classroom for these students?

Educational policies and practices should be fair, compassionate, and effec- tive; and it is difficult to predict if any of these, let alone all three, will be true for a policy we consider using here, as we are planning to implement it here, given the complicated set of interacting factors that are relevant here


The question is not ‘‘What makes a result good evidence?’’ but rather ‘‘What makes a result good evidence for a particular claim?

According to the argument theory, something is evidence for a claim when it serves as a prem- ise in a sound argument for that claim. Sound arguments are composed of trustworthy premises that jointly imply the conclusion.

For this theory too, a research result is evidence relative to a target hypoth- esis and to a set of additional claims describing material facts about the world, including often general truths.

a. We only learn average results. Individuals in the study will generally have differed in their responses to the intervention, and some may even have been harmed by it.
b. Exactly what the control group received matters significantly to the size of the measured treatment effect; the effect will seem much bigger if the control group received an intervention that performs badly than if it received one that performs well.
c. The estimate is, in technical language, unbiased. This has nothing to do with how close it is to the truth.7
d. The conclusion can at best be a causal ascription

Consider the oft-cited case of the color of swans. Multiple samples consisting of only swans in London’s Regents Park do not license the inference that all swans are white.

Researchers work within constraints that impose criteria for selecting study populations. For instance, researchers might be confined to a school district. Within the district, they must conduct the study within schools that are willing and able to participate. Even if all schools in the district are obliged to participate if randomly selected, that dis- trict becomes the parent population. The district itself was not randomly selected, so the experiment cannot support generalizing average effect sizes beyond the district.

Qualitative methods like case studies and ethnographies can illuminate social structures and analyze their interactions with processes or programs.

We argue that, at best, educational RCTs evidence causal ascriptions, which, without further assumptions, are irrelevant to general effectiveness claims and effectiveness predictions.


# Comments

Interesting view on induction and generalization.

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q104475823)  
 * [Wikidata](https://www.wikidata.org/wiki/Q104475823)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q104475823/P921%3BP4510)  
