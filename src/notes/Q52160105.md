
HighLife: Higher-arity Fact Harvesting
======================================
  
  [@wikidata:Q52160105]  
  
Publication date : 01 of April, 2018  

# Highlights

Text-based knowledge extraction methods for populating knowl-edge bases have focused on binary facts: relationships between twoentities. However, in advanced domains such as health, it is oftencrucial to consider ternary and higher-arity relations. An exam-ple is to capture which drug is used for which disease at whichdosage (e.g. 2.5 mg/day) for which kinds of patients (e.g., childrenvs. adults). In this work, we present an approach to harvest higher-arity facts from textual sources.

The largest publicly available KBs are babelnet.org, dbpedia.org, wikidata.org and yago-knowledge.org.They contain many millions of entities and billions of facts

How-ever, a major limitation is that almost all of their facts refer tobinaryrelations only, in the form of subject-predicate-object (SPO) triplesfollowing the RDF data model. For example, DBpedia knows thatMarie Curie has won the Nobel Prize in Physics, but it does not haveany knowledge on which contribution it was for

he following examples demonstratethis by text snippets that contain ternary or quaternary facts onprizes, business acquisitions and health (with relevant argumentsfor relations underlined).

In 1978, Carl Sagan won the Pulitzer Prize for The Dragons of Eden.

However, this pattern collec-tion is small and relies on manual curation. In contrast, our workis automated (with minimal supervision), scales well and can ro-bustly cope with inputs that contain some but not all arguments ofa higher-arity fact.

We are ableto harvest higher-arity relations by utilizing more complex patternrepresentations, i.e. trees instead of pure sequence patterns, and byconsidering partial facts, i.e. facts with unknown arguments.


nowledge Bases:Contrary to knowledge bases, such as YAGO [18],WikiData [48] and Freebase, which extract n-ary facts from pre-structured resources (e.g. Wikipedia Infoboxes) or rely on humaninput, we focus on harvesting n-ary facts from text.

--> Shows little knowledge of how Wikidata works

The goal of the HighLife system is to harvest n-ary facts from textcorpora. One key feature is composing higher-arity facts from par-tial observations by joining arguments, e.g. one sentence referringto a drug, a disease and a target group and another one referring tothe same drug, same target group, a dosage but not the disease arejoined into a single fact containing all 4 pieces of information

Example 1.For the health domain, consider the relation
DruдT reatsDisease:druд×disease×dosaдe×tarдetдroupAn example fact isDrugTreatsDisease(Albuterol, exacerbations, 2.5 mg, children)

--> Elaborate, but still very primitive.

AthleteWonAward(Kerber, Olympic Silver, tennis, 2016, Rio)

--> That is _very_ different from the example before


Aspreprocessing, Stanford CoreNLP software is applied on all texts.Biomedical EntitiesWe rely on the Unified Medical LanguageSystem (UMLS) as biomedical entity dictionary, covering 3,221,702entities with 12,842,558 names. To efficiently find matching candi-dates we employ a method using locality sensitive hashing with min-wise independent permutations. Type information and UMLS’esranked list of entity preferences are used to disambiguate betweenmultiple candidates matched to the same noun chunk.

Numerical quantities are important quantifiers for manyrelations. Our system detects such information in text using power-ful regular expressions incorporating entity types, POS tags, wordsand word classes. We developed a small set of expressions to detectquantities such as prices, percentages, and measurements amongothers. For instance, the expressionword:/USD|$/ [ word:IS_NUM | ner:MONEY ]+denotes dollar prices such asUSD 1 billion.YAGO EntitiesTo recognize and disambiguate entities in news weapply the AIDA system [19] which links entities to YAGO [18].WordNet ConceptsWe apply a most frequent sense disambigua-tion to map remaining noun chunks to concepts in WordNet.Temporal Expressions

Using Stanford’s CoreNLP sutime modulewe detect and normalize time expressions

Consequently, the goals ofthe tree analysis component are to generalize the harvested trees, todetermine salient seed trees that syntactically and lexically expressn-ary relations with high confidence, and to determine n-ary factcandidates from such trees

Datasets.We run experiments on two different input corpora:•News articles:a large collection of news articles, compiledfrom the STICS project [17] and the New York Times archive.•Biomedical texts:a large and diverse collection of documentson biomedicine and health, consisting of i) PubMed abstracts(Medline) as well as entire articles (Central) with scientific con-tent and specialized jargon, and ii) Web portals and encyclopedicarticles (from MayoClinic, Wikipedia, etc.) with informationgeared for patients and doctors (see [14])

For therelationAthleteW onAward, we gathered the seed facts from theWikiData knowledge base. WikiData stores the events (e.g. 2016Summer Olympics) an athlete participates in together with medalswon and the specific date. Combining this with other WikiDatafacts, such as the type of sport an athlete performs and the loca-tion of the event (e.g. Rio for the 2016 Olympics), we constructedinstances of the 6-aryAthleteW onAwardrelation
# Comments


## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q52160105)  
 * [Wikidata](https://www.wikidata.org/wiki/Q52160105)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q52160105/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q52160105&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1145/3178876.3186000)  
