
DrNote: An open medical annotation service

==========
  
  [@wikidata:Q113652930]  

# Highlights

Although various works for data like electronic health reports are available for English texts, only limited work on tools for non-English text resources has been published that offers immediate practicality in terms of flexibility and initial setup. 
We introduce DrNote, an open source text annotation service for medical text processing. Our work provides an entire annotation pipeline with its focus on a fast yet effective and easy to use software implementation.

Further, the software allows its users to define a custom annotation scope by filtering only for relevant entities that should be included in its knowledge base.

The fully automated pipeline can be easily adapted by third parties for custom use cases or directly applied within minutes for medical use cases.
It significantly lowers the barrier for fast analysis of unstructured clinical text data in certain scenarios.

Open datasets of biomedical texts and clinical letters for English languages have been published [49, 50]. In the particular case of German data resources for clinical letters, the situation is more dire [30, 51, 52] as no large dataset is publicly available.

Use of public data: The annotation tool relies on the publicly available, open WikiData and Wikipedia datasets. The datasets are used for initial training of the annotation candidate classifier at build time, and as a knowledge base for entity linking later during the text annotation.

Language support: Our implementation is capable of adopting other languages in its build pipeline. The user can choose a specific language from the set of supported languages in Wikipedia.

Given by the nature of its graphical representation, the WikiData repository can be queried through a public SPARQL-API. The entire WikiData knowledge base is also accessible through a file download for local use.

In this work, we heavily rely on OpenTapioca [53] for solving the entity linking objective. OpenTapioca leverages the tagging functionality of the Apache Solr software in order to implement the candidate generation step. OpenTapioca creates and prepares a Solr collection in advance to index all relevant terms of the WikiData knowledge base for accelerated mention lookup and tagging.

To avoid irrelevant text sections, only the sentences with relevant terms are further extracted. For sentence splitting, SpaCy [16] is used. The transformed data is stored in the common NIF format.

The initialization steps for OpenTapioca are performed as follows: Based on the given OpenTapioca profile, all labels and alias terms of the selected WikiData items are loaded and indexed by the Solr instance. In addition, the buildup of the entity graph for the PageRank computation as well as the buildup of the uni-gram language model is run, followed by the training of the SVM classifier on the extracted NIF dataset. The logical data flow process is visualized in the box Build Stage of Fig 1.

Therefore, PDF documents with digital text information can be directly processed. In case of scanned documents that encode their information in an image format, an additional OCR step is applied. The OCR step is based on the Tesseract [54] software.

One of the most relevant information in medical letters includes data which is associated to symptoms, diagnoses, drugs and medications. Therefore, the entity selection process is managed in the way to cover all WikiData items that represent direct or indirect instances of these concepts in the knowledge base.

An item is a part of the knowledge base index if at least one of the following conditions is satisfied:

* The item has a Disease Ontology (P699) statement entry.
* The item has an UMLS CUI (P2892) statement entry.
* The item has a MeSH descriptor ID (P486) statement entry.
* The item has a MeSH tree code (P672) statement entry.
* The item is a subclass of Medication (P12140).

The build pipeline was executed for an German OpenTapioca profile with the formerly mentioned item selection features. The processing and training was performed on an 8-core Intel Xeon Silver 4210 virtual machine with 128GB memory. 

Since our method is designed for multilingual use cases and for non-English data in specific, we focus on German text data for performance comparisons. 

For instance, the item Universe (Q1) contains the word all as an alias value. Subsequently, all will be linked to Q1 in the case that Q1 was previously included by the entity selection step.



# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q113652930)  
 * [Wikidata](https://www.wikidata.org/wiki/Q113652930)  
 * [Author Disambiguator](https://author-
disambiguator.toolforge.org/work_item_oauth.php?id=Q113652930&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
