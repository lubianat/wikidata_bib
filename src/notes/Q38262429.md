
Using the wisdom of the crowds to find critical errors in biomedical ontologies: a study of SNOMED CT.
======================================================================================================
  
  [@wikidata:Q38262429]  
  
Publication date : 23 of October, 2014  

# Highlights

The crowd identified errors as well as any
single expert at about one-quarter of the cost. The interrater agreement (κ) between the crowd and the experts
was 0.58; the inter-rater agreement between experts
themselves was 0.59, suggesting that the crowd is
nearly indistinguishable from any one expert.
Furthermore, the crowd identified 39 previously
undiscovered, critical errors in SNOMED CT (eg, ‘septic
shock is a soft-tissue infection’).

SNOMED CT (Systematized Nomenclature of Medicine Clinical Terms), an
ontology that the US government now mandates
for use in the clinic as part of ‘Meaningful Use’ of
electronic health records.


Ontologies are vital to many industries, from
e-commerce to biomedicine, from education to
security to e-science. For example, ontologies
support indexing systems such as the Google
Knowledge Graph and Medline. Ontologies
provide a generalized, portable, and reusable
method to apply knowledge computationally, while
abstracting that knowledge from any particular
implementation

The application of ontology in healthcare dates
back to 1893, with the introduction of the
Bertillon Classification of Causes of Death, now the
International Classification of Diseases (ICD),


We devised a method for scalable ontology verification that integrates several approaches from crowdsourcing research into a
unified framework (figure 1). We applied this method as part of
a prospective study wherein the crowd verified relationships
between entities in SNOMED CT,

Specifically, we began with the SNOMED CT CORE Problem
List Subset (http://www.nlm.nih.gov/research/umls/Snomed/
core_subset.html), a subset of SNOMED CT. The CORE subset
is a selection of the most frequently used terms and concepts
across multiple large US healthcare providers

--> interesting for Wikidata


We then devised a micro-task with which to perform verification. We presented each worker (either the crowd or expert)
with concept definitions and an English language statement of a
relationship (figure 3). In previous work, we determined the
optimal fashion in which to present this task to a worker.21 The
worker then indicated whether the statement was correct or
incorrect, thereby verifying the ontological relationship. We
recruited a crowd workforce through CrowdFlower, an online
meta-platform with access to a large online labor force.


Twenty-five workers verified each individual
relationship for a total cost of US$0.50/relationship.

Concurrently, we asked a panel of five experts (MJ, EPM, MAM,
ALR, and TES) in both medicine and ontology to perform the same
verification task that the crowd performed. These experts are representative of domain experts who assist with the development and
maintenance of biomedical ontologies.

. Finally, in comparison with cost of the crowd at US$0.50/relationship, experts cost
∼US$2.00/relationship, based on average task completion time
(4.5 h) and average salary of a general practice physician in
California (∼US$182 580).

The errors the crowd identified are particularly interesting
because they involve concepts in the SNOMED CT CORE
Subset, indicating (1) that these concepts are used very frequently
across many hospitals, and (2) that these concepts and relationships will likely play a role in the clinical decision support
systems required by Meaningful Use. To illustrate the significance
of the errors identified, we describe two hypothetical situations
focused on ‘short-sleeper is a kind of brain disorder.’
A. A clinical decision support system suggests the immobilization
of all persons with a brain disorder. Using the error above, the
system would improperly recommend the immobilization of
those who experience shortened sleep. This incorrect recommendation would certainly cost practitioner time and trust and
may even cause an unwarranted procedure.


e have shown that
crowdsourcing, which researchers use to provide solutions to
intuitive tasks in a scalable way, can address this engineering
challenge. We used crowdsourcing methods to solve the difficult
task of identifying errors in SNOMED CT, an important, large
biomedical ontology. We then compared results from the crowd
with those offered by medical experts who performed the same
task, and we found that errors that the two groups identified
were concordant. The results suggest that crowdsourcing may
offer mechanisms to solve problems that require considerable
biomedical expertise.

--> - 3.2.2. Community curation and gamified science

# Comments

## Tags
- 1.2.2. Computational ontologies and their methods
- 3.2.2. Community curation and gamified science

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q38262429)  
 * [Wikidata](https://www.wikidata.org/wiki/Q38262429)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q38262429/P921%3BP4510)  
