
Creating and Querying Personalized Versions of Wikidata on a Laptop
===================================================================
  
  [@wikidata:Q108208862]  
  
Publication date : 06 of August, 2021  

# Highlights

 None of these methods can support complex, yet common,
query use cases, such as retrieval of large amounts of data or aggregations
over large fractions of Wikidata. This paper introduces KGTK Kypher, a
query language and processor that allows users to create personalized
variants of Wikidata on a laptop. We present several use cases that il-
lustrate the types of analyses that Kypher enables users to run on the
full Wikidata KG on a laptop, combining data from external resources
such as DBpedia.

The Kypher queries for these use cases run much faster
on a laptop than the equivalent SPARQL queries on a Wikidata clone
running on a powerful server with 24h time-out limits.


The key advantages of Kypher over existing tooling are:
1. Ability to extract large amounts of data from Wikidata.
2. Ability to execute queries that retrieve large portions of the full Wikidata.
3. Ability to build personalized versions of Wikidata, and extending it with
other datasets for specific use cases.
4. Easy installation as there are no databases to set up or administer.
5. Minimal hardware requirements, as Kypher can be used to query Wikidata
and DBpedia on a laptop


 KGTK represents KGs in tab-separated (TSV)
files with four columns: edge-identifier, head, edge-label, and tail. All KGTK
commands consume and produce KGs represented in this format, so they can
be composed into pipelines to perform complex transformations on KG

OpenCypher5 is a corresponding open-
source development effort for Cypher which forms the basis of the new Graph
Query Language (GCL).6 We chose Cypher since its ASCII-art pattern language
makes it easy even for novices to express complex queries over graph data.

 Kypher
has been successfully tested on Wikidata-scale graphs with 1.5B edges where
queries executing on a standard laptop run in milliseconds to minutes depend-
ing on selectivity and result sizes. Additional information about Kypher and its
capabilities can be found online.


this use case cannot be implemented
using the public Wikidata endpoint because the query times out, but the Kypher
query runs on a laptop (8 minutes, 16 seconds). The companion Jupyter note-
book illustrates how this query can be extended to measure the popularity of
first names over time.

essicaâ€™s Kypher query is efficient because she built a personalized version of
Wikidata on her laptop, choosing to add the entity count property to her KG
to make other queries run quickly. Using the file as input to her query was all
that Jessica had to do to add the data to her personalized version of Wikidata.
Kypher automatically loaded and indexed the data; in addition, Kypher will
check whether the files on disk have changed every time it runs a query that
uses the file, and will automatically reload and re-index the data as necessary.



he query reuses the p31 and p279star
files to retrieve all publications that are instances of of any subclass of Q591041
(scientific publication). He uses the P50 property to retrieve the authors and
uses two variables (author1 and author2) to retrieve multiple authors if they
are presen

4.4 Queries combining multiple resources
Abigail is working on a cultural heritage project, collaborating with the Getty
Research Institute who gave her a file with 27 thousand identifiers of artists that
she is interested in; the file has one identifier per line. The Getty uses ULAN
identifiers9, and Abigail has a database indexed using VIAF identifiers.10 Abigail
needs to map the ULAN identifiers to VIAF identifiers so that she can use her
database. She puts one of the ULAN identifiers in the Wikidata search box and
discovers that Wikidata has both ULAN and VIAF identifiers for many artists


his solution would require sending
27,000 queries to Wikidata (or 27 queries binding 1000 identifiers), and would
involve writing a script. She writes a SPARQL query that binds all 27,000 identi-
fiers, but the query is too large and it is rejected in the public SPARQL endpoint.

Abigail solves the problem using Kypher. 

Abigail downloads the DBpedia infobox data in RDF format
from the DBpedia Databus11 and uses KGTK commands to convert the data
to KGTK format. The resulting KGTK file contains close to 100 million edges
but the data is noisy as illustrated in the following excerpt. Abigail expects the
node2 column for these properties to contain Wikidata q-nodes, but sees that
often, literals are present.

The main objective of KGTK and Kypher is to democratize the exploitation of
Wikidata so that anyone with modest computing resources can take advantage of
the vast amounts of knowledge present in Wikidata.


Kypher is not meant to address use cases that require the most up-to-date
data in Wikidata. KGTK uses the Wikidata JSON dumps published every few
days, and the KGTK workflow to process the JSON dump takes one day.
 Here we speculate on

the reasons why Kypher seems to perform significantly better than the Wikidata
SPARQL endpoints on the presented use cases:

1. Compact data model: the KGTK data model allows us to translate 1.2B
Wikidata statements very directly into 1.2B edges, while the RDF transla-
tion requires reification and generates O(10B) triples. KGTK also does not
require the use of namespaces which makes data values more compact.

2. Smaller database size: more compact data translates directly into smaller
database sizes, for example, 142GB for the Kypher graph cache vs. 718GB
for the local Wikidata endpoint. This gives generally better locality for table
and index lookups and better caching of data pages.

3. Specialized tables: representing specialized data slices such as P279star in
their own graph tables makes their reuse very efficient and their indexes
more focused, compact, and cache-friendly.

4. Read-only processing: Kypher does not need to support fine-grained updates
of tables and indexes, which need to be supported by the public Wikidata
endpoint. This requires additional machinery that slows down performance.

5. Use case selection: triple stores and databases are optimized to support a
large number of use cases. Our set of use cases samples a small slice of that
space, and performance might be very different for other types of queries.


# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q108208862)  
 * [Wikidata](https://www.wikidata.org/wiki/Q108208862)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q108208862/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q108208862&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [Full text URL](https://arxiv.org/pdf/2108.07119.pdf)  
