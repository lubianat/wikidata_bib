
Knowledge Graphs 2021: A Data Odyssey
=====================================
  
  [@wikidata:Q108272323]  
  

# Highlights
Over the last 15 years, huge knowledge bases, also
known as knowledge graphs, have been automatically constructed
from web data, and have become a key asset for search engines
and other use cases.


This position paper reviews these advances and
discusses lessons learned. It highlights the role of "DB thinking"
in building and maintaining high-quality knowledge bases from
web contents.


Industrial KBs, deployed at major companies and widely
referred to as knowledge graphs (KGs), have an even larger scale,
with one or two orders of magnitude more entities and facts [23].

In addition to general-purpose encyclopedic knowledge (i.e., the
“gist” of Wikipedia contents), there is also notable work on building
domain-specific KGs for verticals like health and life sciences, food
and nutrition, finance, consumer products, and more. Comprehen-
sive surveys on KB construction and curation are [11, 25, 38].


The term knowledge graphs is actually a misnomer and oversim-
plifies the structure and value of KBs. 


Graphs are binary relations,
but KBs are not limited to such instances, called subject-predicate-
object triples, or SPO triples for short.


To demonstrate this necessity, assume that a KB has 90% precision;
with 1 billion statements this means 100 million errors. At this scale,
fixing errors by curators or crowdsourcing is prohibitive.


Likewise, the type taxonomy of a KB needs to be rigorous. Once
we include loose associations as class memberships or subclass-
superclass subsumptions, all kinds of errors are possible. For exam-
ple, including Alan Turing in the class marathon (as he was indeed
a marathon runner), would put him also in the superclass Greek
inventions, and placing the class code breaker as a subclass of
Internet crime would lead to equally wrong inferences.

For entities and types, the UMLS
thesaurus and the MeSH taxonomy are good starting points, sup-
plemented with biomedical databases on drugs (e.g., Drugbank),
proteins (e.g., UniProt) and more.


For example, the following state-
ments would be considered highly notable by most humans (and
are prominently mentioned in Wikipedia articles):
•The Joan Baez song “Diamonds and Rust” is about Bob Dylan.
•A replica of the black monolith from the “2001” movie was found
in a remote canyon in the Utah desert.
•Cixin Liu’s book Three Body features interesting locations like
Tsinghua University and Alpha Centauri.
•Frida Kahlo, the surrealistic Mexican painter, suffered her whole
life from injuries in a bus accident.


Wikidata contains several thousand marathon runners
but knows their best times only for a few tens (not to speak of
all their races)

The bottom line is that neural language models are amazing data
resources of great value and versatility.

earch engines handle quantity lookups for given entities fairly
well (e.g., “Brigid Kosgei personal best”), but largely fail on quantity
filter queries (e.g., “. . . under 2:25:00”


After nearly two decades on research and industrial practice with
automatically constructed knowledge bases, this technology has
become fairly mature. DB thinking, about data quality and consis-
tency constraints, has played a substantial role in these advances.



# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q108272323)  
 * [Wikidata](https://www.wikidata.org/wiki/Q108272323)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q108272323/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q108272323&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
