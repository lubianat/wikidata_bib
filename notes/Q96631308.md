
Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework
====================================================================================================================
  
  [@wikidata:Q96631308]  
  
Publication date : 25 of June, 2020  

# Highlights

To assess the reproducibility
of previously published results, we re-implemented and evaluated 21
models in the PyKEEN software package

We have made all code, experimental configurations, results, and analyses available at https://github.
com/pykeen/pykeen and https://github.com/pykeen/benchmarking.

Knowledge graph embedding models
(KGEMs) present an avenue for predicting missing links.
However, the following two major challenges remain in
their application.

3 KNOWLEDGE GRAPH EMBEDDING MODELS
Knowledge graph embedding models (KGEMs) learn latent
vector representations of the entities e ∈ E and relations
r ∈ R in a KG that best preserve its structural properties [1],
[11], [14]. B


Here, we define a KGEM as four components: an interaction model, a training approach, a loss function, and its
usage of explicit inverse relations. 

3.2.1 Local closed world assumption
The LCWA was introduced by [34] and used in subsequent
works as an approach to generate negative examples during
training [37], [30]. In this setting, for any triple (h, r, t) ∈ K
that has been observed, a set T
−(h, r) of negative examples is created by considering all triples (h, r, ti) ∈ K / as
false. 


# Comments
No mention of Wikidata :(
## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q96631308)  
 * [Wikidata](https://www.wikidata.org/wiki/Q96631308)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q96631308&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [arXiv ID](https://arxiv.org/pdf/2006.13365.pdf)  
