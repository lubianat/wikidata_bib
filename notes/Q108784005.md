
Overview of BioASQ 2021: The Ninth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering
======================================================================================================================
  
  [@wikidata:Q108784005]  
  
Publication date : 01 of January, 2021  

# Highlights

This paper presents an overview of the ninth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2021.

In total, 42 teams with more than 170 systems were registered to participate in the four tasks of the challenge.

The aim of Task 9a is to classify articles from the PubMed/MedLine1 digital library into concepts of the MeSH hierarchy.

Similarly to the previous years, for each test set, participants are required to submit their answers in 21 h.

Task 9b is divided into two phases: (phase A) the retrieval of the required information and (phase B) answering the question. Moreover, it is split into five independent bi-weekly batches and the two phases for each batch run during two consecutive days. 

In phase B, the manually selected relevant articles and snippets for these 100 questions are also released and the participating systems are asked to respond with exact answers, that is entity names or short phrases, and ideal answers, that is natural language summaries of the requested information.


MESINESP was created in response to the lack of resources for indexing content in languages other than English, and to improve the lack of semantic interoperability in the search process when attempting to retrieve medically relevant information across different data sources.


Clinical Trials subtrack (MESINESP-T) asked participating teams to generate models able to automatically predict DeCS codes for clinical trials from the REEC database

In task Synergy the participating systems were expected to retrieve documents and snippets, as in phase A of task B, and, at the same time, provide answers for some of these questions, as in phase B of task B. In contrast to task B, it is possible that no answer exists for some questions.

Also in contrast to task B, during the first round no golden documents and snippets were given, while on the rest of the rounds a separate file with feedback from the experts, based on the previously submitted responses, was provided.

Regarding the ideal answers, the systems are ranked according to manual scores assigned to them by the BioASQ experts during the assessment of systems responses as in phase B of task B [9].

For the exact answers, which are required for all questions except the summary ones, the measure considered for ranking the participating systems depends on the question type. For the yes/no questions, the systems were ranked according to the macro-averaged F1-measure on prediction of no and yes answer. For factoid questions, the ranking was based on mean reciprocal rank (MRR) and for list questions on mean F1-measure.

In this direction, we introduced the BioASQ Synergy task envisioning a continuous dialog between the experts and the systems. In this model, the experts pose open questions and the systems provide relevant material and answers for these questions. Then, the experts assess the submitted material (documents and snippets) and answers, and provide feedback to the systems, so that they can improve their responses. This process proceeds with new feedback and new predictions from the systems in an iterative way.

The team of Roche and Bogazici University participated in task 9a with four different systems (“bert_dna” and “pi_dna” variations). 

The Fudan University (“dmiip_fdu”) team also relied on their previous “AttentionXML” [1], “DeepMeSH” [30], and “BERTMeSH” models [46]. 

The “UCSD” team [27] participated in both phases of the task with two systems (“bio-answerfinder”). Specifically, for phase A they relied on previously developed Bio-AnswerFinder system [28], but instead of LSTM based keyword selection classifier, they used a Bio-ELECTRA++ model based keyword selection classifier together with the Bio-ELECTRA Mid based re-ranker [26].

MESINESP track received greater interest from the public in this second edition. Out of 35 teams registered for CLEF Labs 2021, 7 teams from China, Chile, India, Spain, Portugal and Switzerland finally took part in the task. 


As suggested by Demšar [12], the appropriate way to compare multiple classification systems over multiple datasets is based on their average rank across all the datasets.

--> Well, Demsar has changed his opinion

4.2 Task 9b

Phase A: The evaluation of phase A in Task 9b is based on the Mean Average Precision (MAP) measure for each of the three types of annotations, namely documents, concepts and RDF triples. -

The overall shift of participant systems towards deep neural approaches observed during the last years, is even more apparent this year.

 Therefore, BioASQ keeps pushing the research frontier in biomedical semantic indexing and question answering, extending beyond the English language, through MESINESP, and beyond the already established models for the shared tasks, by introducing Synergy.

 
# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q108784005)  
 * [Wikidata](https://www.wikidata.org/wiki/Q108784005)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q108784005/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q108784005&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1007/978-3-030-85251-1_18)  
