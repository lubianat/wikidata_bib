
Special Issue on Semantic Publishing with Formalization Papers
==========
  
  [@wikidata:Q111482958]  
  

# Highlights

This is at least partially due to the fact that we are still holding on to an archaic paradigm of scientific publishing: the canonical way to publish scientific results is by writing them up in long English texts called articles, which are in the best case easy to read by human experts but remain mostly inaccessible to automated approaches (except on a very superficial level with text mining techniques).

 An important aspect of this is the vision of semantic publishing, which we mean here in the sense of genuine semantic publishing [9], where the machine-interpretable formal semantics cover the main scientific claims the work is making. Nanopublications [8], which are small RDF-based semantic packages, have emerged as a powerful concept and technology for enabling such genuine semantic publishing.

 -->   - 1.1.3. Interoperable publication processes (nanopublications) 

Taking an example from our previous study as illustration of the super-pattern, it has been stated in the scientific literature [7] that in particular kinds of cells in the rat brain (specifically, endothelial cells) some sort of stress called transient oxidative stress affects the expression of a protein called Pgp. The super-pattern consists of five slots that would in this example be filled in as follows:

Context class: rat brain endothelial cell

Subject class: transient oxidative stress

Qualifier: generally

Relation: affects

Object class: Pgp expression

Informally, we can read this in the following way: whenever there is an instance of transient oxidative stress in the context of an instance of a rat brain endothelial cell, then generally (meaning in at least 90% of the cases), that instance of stress has the relation of affecting an instance of Pgp expression. Formally, it directly maps to this logic formula:

P(∃z(pgp-expression(z)∧ in-context(z,x)∧affects(y,z))|transient-oxidative-stress(y)∧ rat-brain-endothelial-cell(x)∧in-context(y,x))⩾0.9

This is stating in logic terms (in slightly non-standard notation using conditional probability as a shorthand) that given a thing y of type transient-oxidative-stress in the context of a thing x of type rat-brain-endothelial-cell, the probability of there being a z of type pgp-expression that is in the same context x is at least 90%. It has been shown that this pattern can be applied to formalize most high-level claims found in scientific literature across disciplines [4].


For practical reasons, we did not require the scientific claims to be novel ones, but they were selected from existing publications. This special issue consists of what we call formalization papers, which are nanopublication-based semantic publications whose novelty lies in the formalization of a previously published scientific claim.

 They should be fully semantically represented (in RDF) but also have classical views that makes them look like other articles. 

 A formalization paper contributes a semantic formalization of one of the main claims of an already published scientific article. 

 In order to publish formalization papers, class definitions, and all the other kinds of nanopublications (submissions, reviews, responses to reviews, and decisions), Nanobench [10]3 was used.

All the application-specific behavior is therefore semantically represented in the templates, and Nanobench can flexibly be used for any other kind of data and workflow.

It is a simple user interface component built on top of grlc [12] that allows to run template-based SPARQL queries on RDF triple stores. 

Interested authors could submit formalization papers, which upon acceptance were published in this special issue. The goal of this was to demonstrate for the first time that scientific articles can be formalized and therefore machine-interpretable including the main scientific claims.

The authors of formalization papers formalized their own previously published claim, or a claim from a paper published by others. In the latter case, the formalization paper authors take credit for the formalization of the claim but not for the claim itself.

All submissions to this special issue were peer-reviewed (also as nanopublications) using a previously proposed reviewing ontology [2].

 Here, the classes used to instantiate the super-patterns that comprise the formalizations are given for each submission: the context, subject and object classes for each submission are listed, together with the qualifier and relations selected from the SuperPattern ontology [4].

 Additionally, out of the total 44 classes used in the formalizations, 22 new classes were minted using Nanobench (marked with *), while 4 were newly minted Wikidata classes (marked with **). 13 already-existing classes were reused from Wikidata (their Wikidata identifier is specified next to the class name) and 4 classes were referenced from other ontologies.

 -->     - 1.3.3. Wikidata as a platform for representation of biological knowledge:

Retrieve all genes that have been found to play a role in a part of the respiratory system in Covid-19 patients. Only include results from randomized controlled trials published in the last seven days.

To a naive developer without experience in how scientific knowledge is communicated, this might sound quite easy. One would just have to find the right API, translate the task description into a query, and possibly do some post-processing and filtering. But everybody who knows a bit how science is communicated immediately realizes that this will take a much bigger effort.

text mining is not good enough, and probably will never be.

Just to detect the type of a relation between a given drug and a given gene out of 13 given relation types, the best system achieved an F-score of 0.7973.

The task focussed on extracting chemicals, and this is done it a two-stage process. First the entities are recognized in the text, with an F-score of the best system of 0.8672, and then the recognized chemicals are linked to the corresponding formal identifiers, with the best F-score being 0.8136:

An overall F-score of 0.40, as resulting from this calculation, roughly means that around 60% of retrieved relations are wrong and 60% of existing relations are not retrieved. 

To better understand the logical structure of such high-level scientific findings (e.g. that a gene tends to have a certain effect on the course of a given disease), we selected a random sample of 75 research articles from Semantic Scholar.

To represent these findings and thereby the formalization papers, we used the nanopublication format and the Nanobench tool. Researchers who contributed to this special issue filled in a form to express and submit their formalization that looked like this:

We ended up with 15 formalization papers in our special issue, as summarized by this table:

So, for the first time, software can reliably interpret the main high-level findings of scientific publications. 
This special issue is a just a small first step, but it could prove to be the first step into a new era of scientific publishing. The practical immediate consequences of this might seem limited, but I think the longer-term potential of making scientific knowledge accessible to the interpretation by machines is hard to overstate.




# Comments

Related blogpost: https://gist.github.com/tkuhn/1a99ce119d248f53689d52239512ec1f

## Highlights




## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q111482958)  
 * [Wikidata](https://www.wikidata.org/wiki/Q111482958)  
 * [Author Disambiguator](https://author-
disambiguator.toolforge.org/work_item_oauth.php?id=Q111482958&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
