
LitSense: making sense of biomedical literature at sentence level
=================================================================
  
  [@wikidata:Q90290716]  
  
Publication date : 01 of July, 2019  

# Highlights

, LitSense returns best-matching sentences using both a traditional term-weighting approach that up-weights sentences that contain more of the rare terms in the user query as well as a novel neural embedding approach that enables the retrieval of semantically relevant results without explicit keyword match. LitSense provides a user-friendly interface that assists its users to quickly browse the returned sentences in context and/or further filter search results by section or publication date. 

LitSense also employs PubTator to highlight biomedical entities (e.g. gene/proteins) in the sentences for better result visualization. 

--> 3.2. Community annotation of texts via Wikidata

As shown in Figure 1, LitSense has two major components: ‘sentence indexing’ and ‘sentence search’. We pre-process all sentences from PubMed and PMC and then index them in Sol
During search time, for a user query, LitSense first returns sentences that best match the query terms from the Solr database. The retrieved sentences are then re-ranked using semantic vectors. This re-ranked result is displayed to the user in the last step.


Although term-based matching (i.e. IDF) is efficient for searching a half-billion sentences, it may not fully capture the underlying semantics of natural language (21). That is, two sentences that share the same terms may not be similar in semantics. Recent sentence-embedding approaches aim to address this problem by producing sentence vectors that capture the semantics beyond word level (22,23)


hen, we re-rank the N sentences by averaging IDF and sent2vec similar scores (Figure 1C). Note that we take an average of IDF and sent2vec empirically, following the experiments described below.

--> Cool approach


We developed LitSense using the Angular framework (https://angular.io) for the frontend and the Django framework (https://www.djangoproject.com) for the backend

Case 2: Biocurators perform evidence attribution

Evidence attribution is an essential step in biocuration workflows (34). 

Assume that one read a news article about a measles outbreak in a certain area; the user might want to try to find information about measles outbreaks and vaccinations. With the query, ‘measles outbreak vaccination’, LitSense retrieves sentences such as

    Such seasonal outbreak patterns were eliminated in the US after 1981, through the implementation of the highly effective MMR (measles, mumps and rubella) blanket vaccination program (PMCID 5032840).

    

# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q90290716)  
 * [Wikidata](https://www.wikidata.org/wiki/Q90290716)  
 * [TABernacle](https://tabernacle.toolforge.org/?#/tab/manual/Q90290716/P921%3BP4510)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q90290716&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1093/NAR/GKZ289)  
