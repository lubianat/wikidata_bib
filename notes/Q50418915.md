
Towards a Question Answering System over the Semantic Web
=========================================================
  
  [@wikidata:Q50418915]  
  
Publication date : 02 of March, 2018  

# Highlights

Give me actors born in Berlin”. This question can be reformulated in many ways like “In Berlin were born which actors?” or as a keyword question “Berlin, actors, born in”. In this case by knowing the semantics of the words “Berlin”, “actors”, “born”, we are able to deduce the intention of the user. 
Construct a set of queries that represent possible interpretations of the given question within the given KB.
If no training data is available, then we rank the queries using a linear combination of:
Number of the words in the question which are covered by the query.
The edit distance of the label of the resource and the word it is associated to. For example, the edit distance between the label of dbp:bornYear (which is “born year”) and the word “born” is 5.
The sum of the relevance of the resources, (e.g. the number of inlinks and the number of outlinks of a resource).
The computations in the previous section lead to a list of ranked SPARQL queries candidates representing our possible interpretations of the user’s intentions.
4. Fast candidate generation In this section, we explain how the SPARQL queries described in section 3.2 can be constructed efficiently.
We tested our approach on 5 different datasets namely Wikidata10, DBpedia11, MusicBrainz12, DBLP13 and Freebase14.
The property dbo:birthPlace has only one lexicalization namely “birth place”, while the corresponding property over Wikidata P19 has 10 English lexicalizations.
QALD: We evaluated our approach using the QALD benchmarks. These benchmarks are good to see the performance on multiple languages and over both full natural language questions and keyword questions
The error sources are the following:
 40% are due to lexical gap (e.g. for “Who played Gus Fring in Breaking Bad?” the property dbo:portrayer is expected)
28% come from wrong ranking
12% are due to the missing support of superlatives and comparatives in our implementation (e.g. “Which Indian company has the most employees?”)
9% from the need of complex queries with unions or filters (e.g. the question “Give me a list of all critically endangered birds.” requires a filter on dbo:conservationStatus equal “CR”)
2% from too ambiguous questions (e.g. “Who developed Slack?” is expected to refer to a “cloud-based team collaboration tool” while we interpret it as “linux distribution”).
This project has received funding from the European Union’s Horizon 2020 research and innovation program


# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q50418915)  
 * [Wikidata](https://www.wikidata.org/wiki/Q50418915)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q50418915&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [Full text URL](https://arxiv.org/pdf/1803.00832.pdf)  
 * [arXiv ID](https://arxiv.org/pdf/1803.00832.pdf)  
