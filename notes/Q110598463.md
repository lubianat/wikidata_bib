
Evaluation Measures for Hierarchical Classification: a unified view and novel approaches
========================================================================================
  
  [@wikidata:Q110598463]  
  
Publication date : 28 of June, 2013  

# Highlights

This paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key
components of the existing performance measures. It also proposes two
alternative generic views of hierarchical evaluation and introduces two
corresponding novel measures

many real-world knowledge systems and services use a hierarchical scheme to organize their data (e.g.
Yahoo, Wikipedia)

In contrast to flat classification, where the classes are considered unrelated, in
HC the classes are organized in taxonomies. The taxonomies are usually either
trees, in which case nodes (classes) have a single parent each, or directed acyclic
graphs (DAGs)

A taxonomy is thus usually defined as a pair (C, ≺), where
C is the set of all classes (Silla and Freitas, 2011) and ≺ is the subclass-of

relationship with the following properties:1
• Asymmetry: if ci ≺ cj then cj ⊀ ci for every ci, cj ∈ C.
• Anti-reflexivity: ci ⊀ ci for every ci ∈ C.
• Transitivity: if ci ≺ cj and cj ≺ ck, then ci ≺ ck for every ci, cj , ck ∈ C

In order to measure the severity of an error in hierarchical classification,
there are several interesting issues that need to be addressed. Figure 2 presents
five cases that require special handling.

Figure 2(a) presents an over-specialization error where the predicted class is
a descendant of the true class. 

Figure 2(b) depicts an under-specialization error,
where an ancestor of the true class is selected. In both these cases the desired
behavior of the measure would be to reduce the penalty to the classification
system, according to the distance between the true class and the predicted one.

--> This is way better than 2a. It is technically correct, but likely less useful. 

Figure 2(d) presents a scenario which is common in multi-label data. In this
case one must decide, before even measuring the error, which pairs of true and
predicted classes should be compared.

Finally, Figure 2(e) presents a case where the predicted class should probably
not be matched to any true class. This is typically the case when the predicted
class and the true class are too distant which is why we call this case the long
distance problem.

Pair-based measures typically calculate the cost κij of a pair of a predicted
class ˆyi and a true class yi as the minimum distance of ˆyi and yi
in the hierarchy,
e.g. as the number of edges between the classes along the shortest path that
connects them.

--> The undirectionality kind of bothers me


# Comments

## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q110598463)  
 * [Wikidata](https://www.wikidata.org/wiki/Q110598463)  
 * [Author Disambiguator](https://author-disambiguator.toolforge.org/work_item_oauth.php?id=Q110598463&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [DOI](https://doi.org/10.1007/s10618-014-0382-x)  
 * [Full text URL](https://arxiv.org/pdf/1306.6802.pdf)  
 * [arXiv ID](https://arxiv.org/pdf/1306.6802.pdf)  
