
SPARQL as a Foreign Language
============================
  
  [@wikidata:Q111308293]  
  
Publication date : 25 of August, 2017  

# Highlights

Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.

SINA [11] is a QA system that explores the knowledge base to formulate the SPARQL query by applying Jaccard similarity to evaluate possible resource matching candidates and Hidden Markov Models to choose the right answer among them. In this conversion, queries are identified (query-type) based on pre-defined syntactic rules and resources are linked using DBpedia Spotlight [4]. Recently, [7] proposed a fact retrieval approach based on a Recurrent Neural Network (RNN) encoding subject labels at character level and predicates and datatype values at both character and word levels.

. Below are a few examples of query templates:

where is <A> located in?
  ->  select ?a where { <A> dbo:location ?a }
what are the <A> northernmost <B>?
  ->  select ?a where { ?a rdf:type <B> . ?a geo:lat ?b }
      order by desc(?b) limit <A>   
was <A> finished before <B>?
  ->  ask where { <A> dbp:complete ?a . FILTER(?a <= <B>) }

. For instance, a model trained on DBpedia may learn that "is located in" translates into dbo:country. However, property dbo:country might not have any sense in another dataset where another property is used (e.g., property http://schema.org/Country is used in the Schema.org ontology).

n this work, we presented a neural approach for the translation of NL questions into SPARQL queries using a NN model. Preliminary results show an average BLEU accuracy of ~0.8 when trained on QA template model pairs, even when answering such questions requires the formulation of more complex queries. The current model is vocabulary-dependent and needs to be trained on samples derived from manually-created QA template pairs. We plan to address current limitations by investigating how to generate domain-independent templates and minimize the burden on the end user.

--> Looks over fitted


# Comments
Lust htm paper, interesting: https://arxiv.org/html/1708.07624
## Tags

# Links
  
 * [Scholia Profile](https://scholia.toolforge.org/work/Q111308293)  
 * [Wikidata](https://www.wikidata.org/wiki/Q111308293)  
 * [Author Disambiguator](https://author-
disambiguator.toolforge.org/work_item_oauth.php?id=Q111308293&batch_id=&match=1&author_list_id=&doit=Get+author+links+for+work)  
 * [Full text URL](https://arxiv.org/pdf/1708.07624.pdf)  
 * [arXiv ID](https://arxiv.org/pdf/1708.07624.pdf)  
